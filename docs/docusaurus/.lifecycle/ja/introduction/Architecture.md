---
displayed_sidebar: "Japanese"
---

# アーキテクチャ

StarRocksはシンプルなアーキテクチャを持っています。システム全体は、フロントエンド（FE）とバックエンド（BE）またはコンピュートノード（CN）の2種類のコンポーネントだけで構成されています。StarRocksは外部コンポーネントに依存せず、展開とメンテナンスを簡素化しています。ノードはサービス停止時間なしに水平にスケーリングすることができます。さらに、StarRocksはメタデータとサービスデータのレプリカメカニズムを持ち、データの信頼性を高め、単一障害点（SPOF）を効果的に防止します。

StarRocksはMySQLプロトコルと互換性があり、標準SQLをサポートしています。ユーザーは簡単にMySQLクライアントからStarRocksに接続して、即座かつ価値ある洞察を得ることができます。

## アーキテクチャの進化

StarRocksは進化を続けるにつれ、システムアーキテクチャは元々のストレージ・コンピュートカプルアーキテクチャ（共有なし）からストレージ・コンピュート分離アーキテクチャ（共有データ）へ移行しました。

- StarRocks 3.0より前のバージョンでは、ストレージ・コンピュートカプルアーキテクチャを使用しています。BEはデータのストレージと計算の両方を担当しています。データのアクセスと計算はローカルノードで行われ、データの移動を最小限に抑え、クエリのレイテンシを低減することで、超高速なクエリと分析の体験を提供します。

- StarRocks 3.0以降では、ストレージ・コンピュート分離アーキテクチャが導入されています。データストレージはBEから分離され、BEはステートレスなCNノードにアップグレードされます。データは永続的にリモートオブジェクトストレージまたはHDFSに格納され、CNのローカルディスクはクエリの高速化のためのホットデータのキャッシュに使用されます。ストレージ・コンピュート分離アーキテクチャは、コンピュートノードの動的な追加と削除をサポートしており、オンデマンドのスケーリングが可能です。

以下の図は、アーキテクチャの進化を示しています。

![アーキテクチャの進化](../assets/architecture_evolution.png)

## ストレージ・コンピュートカプル

3.0より前のバージョンでは、StarRocksは典型的なマルチパラレルプロセッシング（MPP）データベースとして、ストレージ・コンピュートカプルアーキテクチャを使用しています。このアーキテクチャでは、BEはデータのストレージと計算の両方を担当しています。BEモード上のローカルデータへの直接アクセスにより、ローカル計算が可能であり、データ転送とデータコピーを回避して、超高速なクエリと分析のパフォーマンスを提供します。このアーキテクチャはマルチレプリカデータストレージをサポートし、高い同時性クエリ処理能力とデータ信頼性を確保します。最適なクエリパフォーマンスを追求するシナリオに適しています。

### ノード

ストレージ・コンピュートカプルアーキテクチャでは、StarRocksは2種類のノードから構成されています：FEとBE。

- FEはメタデータの管理と実行計画の構築を担当しています。
- BEはクエリの実行とデータのストレージを行います。BEはローカルストレージを使用してクエリの高速化を図り、マルチレプリカメカニズムを使用してデータの高可用性を確保します。

### FE

FEはメタデータの管理、クライアント接続の管理、クエリの計画、およびクエリのスケジューリングを担当しています。各FEはメモリに完全なメタデータのコピーを保持し、FE間で均等なサービスを提供することを保証します。FEはリーダー、フォロワー、オブザーバの役割を果たすことができます。フォロワーはPaxosライクなBDB JEプロトコルに従ってリーダーを選出することができます。BDB JEはBerkeley DB Java Editionの略です。

| **FEの役割** | **メタデータの管理**                                               | **リーダー選出**                                              |
| ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| リーダーFE      | リーダーFEはメタデータの読み書きを行います。フォロワーおよびオブザーバFEはメタデータの読み取りのみを行うことができます。彼らはメタデータの書き込みリクエストをリーダーFEにルーティングします。リーダーFEはメタデータを更新し、BDE JEを使用してメタデータの変更をフォロワーおよびオブザーバFEに同期させます。データの書き込みは、メタデータの変更がフォロワーFEの半数以上に同期された後にのみ成功したとみなされます。 | リーダーFEはフォロワーFEから選出されます。リーダー選出を行うためには、クラスタ内のフォロワーFEの半数以上がアクティブである必要があります。リーダーFEが失敗した場合、フォロワーFEは再度リーダー選出を開始します。 |
| フォロワーFE    | フォロワーはメタデータの読み取りのみを行います。彼らはリーダーFEからログを同期し、メタデータを更新します。 | フォロワーはリーダー選出に参加します。これには、クラスタ内のフォロワーが半数以上アクティブであることが必要です。 |
| オブザーバFE   | オブザーバはリーダーFEからログを同期し、メタデータを更新します。     | オブザーバはクラスタのクエリ同時性を増加させるために主に使用されます。オブザーバはリーダー選出に参加せず、クラスタにリーダー選択の圧力を加えることはありません。|

### BE

BEはデータのストレージとSQLの実行を担当しています。

- データのストレージ：BEは同等のデータストレージ機能を持っています。FEは事前定義されたルールに基づいてデータをBEに分配します。BEは受け入れたデータを変換し、必要な形式にデータを書き込み、データのためのインデックスを生成します。

- SQLの実行：SQLクエリが到着すると、FEはクエリの意味に従って論理的な実行計画に解析し、その後、論理計画をBEで実行可能な物理的な実行計画に変換します。目的のデータを保持するBEがクエリを実行します。これにより、データの転送やコピーの必要性がなくなり、高いクエリパフォーマンスが実現されます。

### データ管理

StarRocksは列指向のデータベースシステムです。パーティショニングとバケット分割のメカニズムを使用してデータを管理します。テーブルのデータはまず複数のパーティションに分割され、その後複数のタブレットに分割されます。タブレットはStarRocksのデータ管理の基本的な論理単位です。各タブレットには、異なるBEに分散して保存できる複数のレプリカが存在します。タブレットの数を指定することができ、StarRocksにタブレットの管理を任せることができます。

パーティションとタブレットにより、テーブルのスキャンが減少し、クエリの同時実行性が向上します。レプリカにより、データのバックアップとリストアが容易に行えます。

以下の図では、テーブルが時間に基づいて4つのパーティションに分割されています。最初のパーティションのデータはさらに4つのタブレットに分割されています。各タブレットには3つのレプリカがあり、3つの異なるBEに保存されています。

![データ管理](../assets/data_manage.png)

テーブルが複数のタブレットに分割されているため、StarRocksは1つのSQLステートメントをすべてのタブレットに対して並行処理することができます。これにより、複数の物理マシンやコアの計算能力を最大限に活用することができます。また、クエリの圧力を複数のノードにオフロードするため、サービスの可用性が向上します。高同時性を実現するために、必要に応じて物理マシンを追加することができます。

タブレットの分散は物理ノードに影響を受けることはありません。BEの数が変わった場合（例えば、BEを追加または削除した場合）、稼働中のサービスは中断することなく継続されます。ノードの変更により、タブレットの自動移行がトリガーされます。BEが追加されると、いくつかのタブレットは新しいBEに自動的に移行され、より均等なデータ分布が実現されます。BEが削除されると、これらのBE上のタブレットは他のBEに自動的に移行され、レプリカの数は変わらないようにします。自動タブレット移行は、StarRocksクラスタの自動スケーリングを容易に実現し、手動のデータ再分配の必要性を排除します。

StarRocksはタブレットのためのマルチレプリカメカニズム（デフォルトでは3）を使用しています。レプリカは高いデータの信頼性とサービスの可用性を確保します。1つのノードの故障が全体のサービスの可用性に影響を与えることはありません。また、レプリカの数を増やすことで、高いクエリ同時実行性を実現することもできます。

### 制約事項

このアーキテクチャにはいくつかの制約があります：

- 成長コスト：ユーザーはストレージと計算をスケーリングする必要があり、望ましくないストレージコストが増加します。データ量が増えるにつれて、ストレージと計算リソースの需要が不均衡に増加し、リソースの効率が低下します。
- 複雑なアーキテクチャ：複数のレプリカ間でデータの整合性を維持することは、システムに複雑さをもたらし、障害のリスクを増大させます。
- 限られた弾力性：スケーリング操作によりデータの再バランスが発生し、ユーザーエクスペリエンスが不十分になる場合があります。

## ストレージ・コンピュート分離

新しいストレージ・コンピュート分離アーキテクチャでは、データのストレージ機能がBEから分離されます。BEは「コンピュートノード（CN）」と呼ばれ、データの計算とホットデータのキャッシュのみを担当します。データは低コストで信頼性の高いリモートストレージシステム（Amazon S3、GCP、Azure Blobストレージなど）またはMinIOのようなS3互換ストレージに格納されます。キャッシュがヒットした場合、クエリのパフォーマンスはストレージ・コンピュートカプルアーキテクチャと比較して同等です。CNノードは数秒で追加または削除できます。このアーキテクチャはストレージコストを削減し、リソースの隔離、高い弾力性とスケーラビリティを確保します。

ストレージ・コンピュート分離アーキテクチャは、共有データを含むカプルアーキテクチャと同じシンプルなアーキテクチャを持っています。FEとCNの2種類のノードだけで構成されています。唯一の違いは、ユーザーがバックエンドのオブジェクトストレージを準備しなければならないことです。

![共有データアーキテクチャ](../assets/architecture_shared_data.png)

### ノード

ストレージ・コンピュート分離アーキテクチャのFEは、ストレージ・コンピュートカプルアーキテクチャと同じ機能を提供します。

BEのストレージ機能が分離されました。ローカルストレージは共有ストレージに変わりました。BEノードは状態を持たないCNノードにアップグレードされ、データのロード、クエリの計算、およびキャッシュの管理などのタスクを担当します。

### ストレージ

現在、StarRocksの共有データクラスタは2つのストレージソリューションをサポートしています：オブジェクトストレージ（例：AWS S3、Google GCS、Azure Blobストレージ、MinIOなど）および従来のデータセンターで展開されたHDFS。この技術により、指定されたバケットまたはHDFSディレクトリ内のデータのストレージが統一されます。

共有データクラスタでは、データファイルの形式は共有なしクラスタ（ストレージとコンピュートを結びつけたクラスタ）と同じままです。データはセグメントファイルに組織され、さまざまなインデックス技術がクラウドネイティブテーブル（共有データクラスタで特に使用されるテーブル）で再利用されます。

### キャッシュ

StarRocksの共有データクラスタは、データのストレージと計算を分離して独立してスケーリングできるようにすることで、コストを削減し、弾力性を向上させています。ただし、このアーキテクチャはクエリのパフォーマンスに影響を与える可能性があります。

この影響を軽減するために、StarRocksはメモリ、ローカルディスク、リモートストレージを含むマルチティアデータアクセスシステムを確立して、さまざまなビジネスニーズに対応しています。
```
Queries against hot data scan the cache directly and then the local disk, while cold data needs to be loaded from the object storage into the local cache to accelerate subsequent queries. By keeping hot data close to compute units, StarRocks achieves truly high-performance computation and cost-effective storage. Moreover, access to cold data has been optimized with data prefetch strategies, effectively eliminating performance limits for queries.

Users can decide whether to enable caching when creating tables. If caching is enabled, data will be written to both the local disk and backend object storage. During queries, the CN nodes first read data from the local disk. If the data is not found, it will be retrieved from the backend object storage and simultaneously cached on the local disk.
```