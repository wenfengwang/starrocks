---
displayed_sidebar: Chinese
---

# AWS S3からのインポート

import LoadMethodIntro from '../assets/commonMarkdown/loadMethodIntro.md'

import InsertPrivNote from '../assets/commonMarkdown/insertPrivNote.md'

StarRocksは、以下の方法でAWS S3からデータをインポートすることをサポートしています:

<LoadMethodIntro />

## 準備作業

### データソースの準備

インポートするデータがS3バケットに保存されていることを確認してください。StarRocksクラスタと同じリージョンにあるS3バケットにデータを保存することをお勧めします。これにより、データ転送コストを削減できます。

この記事では、`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`というサンプルデータセットを提供しており、すべての合法的なAWSユーザーがアクセスできます。実際の有効なセキュリティクレデンシャルを設定するだけで、このデータセットにアクセスできます。

### 権限の確認

<InsertPrivNote />

### リソースアクセス設定の取得

この記事の例では、IAM Userベースの認証方式を使用しています。AWS S3に保存されているデータにスムーズにアクセスできるように、「[IAM Userベースの認証と認可](../integrations/authenticate_to_aws_resources.md)」で紹介されている準備作業に従ってIAM Userを作成し、[IAMポリシー](../reference/aws_iam_policies.md)を設定することをお勧めします。

要約すると、IAM Userベースの認証方式を使用する場合、以下のAWSリソース情報を事前に取得する必要があります:

- データが存在するS3バケット
- S3オブジェクトキー（または「オブジェクト名」）（特定のデータオブジェクトにアクセスする場合のみ必要です。サブフォルダ内に保存されているデータオブジェクトの場合、その名前にはプレフィックスを含めることができます。）
- S3バケットが存在するAWSリージョン
- アクセスキーとシークレットキーとしてのアクセス証明

StarRocksがサポートする他の認証方式については、[AWS認証情報の設定](../integrations/authenticate_to_aws_resources.md)を参照してください。

## INSERT+FILES()を使用したインポート

この機能はバージョン3.1からサポートされています。現在、ParquetとORCファイル形式のみをサポートしています。

### INSERT+FILES()の利点

`FILES()`は、指定されたデータパスなどのパラメータを読み取り、データファイルの形式や列情報などに基づいて自動的にテーブル構造を推測し、最終的にファイル内のデータをデータ行として返します。

`FILES()`を使用すると、以下が可能です:

- [SELECT](../sql-reference/sql-statements/data-manipulation/SELECT.md)文を使用して、AWS S3から直接データをクエリします。
- [CREATE TABLE AS SELECT](../sql-reference/sql-statements/data-definition/CREATE_TABLE_AS_SELECT.md)（CTASとも呼ばれます）文を使用して、自動的にテーブルを作成し、データをインポートします。
- 手動でテーブルを作成し、[INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md)を使用してデータをインポートします。

### 操作例

#### SELECTを使用して直接データをクエリする

SELECT+`FILES()`を使用して、AWS S3内のデータを直接クエリし、テーブルを作成する前にインポートするデータについて全体的な理解を得ることができます。その利点は以下の通りです:

- データを保存する必要なく、それを見ることができます。
- データの最大値と最小値を確認し、どのデータタイプを使用するかを決定できます。
- データに`NULL`値が含まれているかどうかをチェックできます。

例えば、サンプルデータセット`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`内のデータをクエリします:

```SQL
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
)
LIMIT 3;
```

> **説明**
>
> 上記のコマンド例の`AAA`と`BBB`を実際の有効なアクセスキーとシークレットキーに置き換えて、アクセス証明として使用してください。ここで使用されているデータオブジェクトはすべての合法的なAWSユーザーに公開されているため、任意の実際の有効なアクセスキーとシークレットキーを入力しても問題ありません。

システムは以下のクエリ結果を返します：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
| 543711 |  829192 |    2355072 | pv           | 2017-11-27 08:22:37 |
| 543711 | 2056618 |    3645362 | pv           | 2017-11-27 10:16:46 |
| 543711 | 1165492 |    3645362 | pv           | 2017-11-27 10:17:00 |
+--------+---------+------------+--------------+---------------------
```

> **説明**
>
> 上記の結果における列名は、元のParquetファイルで定義された列名です。

#### CTASを使用して自動的にテーブルを作成し、データをインポートする

この例は、前の例の続きです。この例では、CREATE TABLE AS SELECT（CTAS）文に前の例のSELECTクエリをネストすることで、StarRocksは自動的にテーブル構造を推測し、テーブルを作成し、新しく作成されたテーブルにデータをインポートできます。Parquet形式のファイルには列名とデータタイプが含まれているため、列名やデータタイプを指定する必要はありません。

> **説明**
>
> テーブル構造の推測機能を使用する場合、CREATE TABLE文ではレプリケーション数を設定できません。したがって、テーブルを作成する前にレプリケーション数を設定する必要があります。例えば、以下のコマンドを使用してレプリケーション数を`1`に設定できます:
>
> ```SQL
> ADMIN SET FRONTEND CONFIG ('default_replication_num' = "1");
> ```

以下の文を使用してデータベースを作成し、そのデータベースに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

CTASを使用して自動的にテーブルを作成し、サンプルデータセット`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`内のデータを新しく作成されたテーブルにインポートします:

```SQL
CREATE TABLE user_behavior_inferred AS
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **説明**
>
> 上記のコマンド例の`AAA`と`BBB`を実際の有効なアクセスキーとシークレットキーに置き換えて、アクセス証明として使用してください。ここで使用されているデータオブジェクトはすべての合法的なAWSユーザーに公開されているため、任意の実際の有効なアクセスキーとシークレットキーを入力しても問題ありません。

テーブル作成後、[DESCRIBE](../sql-reference/sql-statements/Utility/DESCRIBE.md)を使用して新しく作成されたテーブルの構造を確認できます:

```SQL
DESCRIBE user_behavior_inferred;
```

システムは以下のクエリ結果を返します：

```Plaintext
+--------------+------------------+------+-------+---------+-------+
| Field        | Type             | Null | Key   | Default | Extra |
+--------------+------------------+------+-------+---------+-------+
| UserID       | bigint           | YES  | true  | NULL    |       |
| ItemID       | bigint           | YES  | true  | NULL    |       |
| CategoryID   | bigint           | YES  | true  | NULL    |       |
| BehaviorType | varchar(1048576) | YES  | false | NULL    |       |
| Timestamp    | varchar(1048576) | YES  | false | NULL    |       |
+--------------+------------------+------+-------+---------+-------+
```

システムによって推測されたテーブル構造と手動で作成されたテーブル構造を以下の点で比較します：

- データタイプ
- `NULL`値を許可するかどうか
- キーとして定義されたフィールド

本番環境では、ターゲットテーブルの構造をより良く制御し、より高いクエリパフォーマンスを実現するために、手動でテーブルを作成し、テーブル構造を指定することをお勧めします。

新しく作成されたテーブル内のデータをクエリして、データが正常にインポートされたことを確認できます。例えば：

```SQL
SELECT * from user_behavior_inferred LIMIT 3;
```

システムは以下のクエリ結果を返し、データが正常にインポートされたことを示します：

```Plaintext
+--------+--------+------------+--------------+---------------------+
| UserID | ItemID | CategoryID | BehaviorType | Timestamp           |
+--------+--------+------------+--------------+---------------------+
|     58 | 158350 |    2355072 | pv           | 2017-11-27 13:06:51 |
|     58 | 158590 |    3194735 | pv           | 2017-11-27 02:21:04 |
|     58 | 215073 |    3002561 | pv           | 2017-11-30 10:55:42 |
+--------+--------+------------+--------------+---------------------+
```

#### INSERTを使用して手動でテーブルを作成し、データをインポートする

実際のビジネスシナリオでは、ターゲットテーブルの構造をカスタマイズする必要があるかもしれません。これには以下が含まれます：

- 各列のデータタイプ、デフォルト値、および`NULL`値を許可するかどうか
- キーとして定義される列とそのデータタイプ
- データのパーティション分割とバケット分割

> **説明**
>
> 効率的なテーブル構造設計を実現するためには、テーブル内のデータの用途や各列の内容について深く理解する必要があります。この記事ではテーブル設計について詳しくは述べませんが、詳細については[テーブル設計](../table_design/StarRocks_table_design.md)を参照してください。

この例では、AWS S3に保存されているソースファイルのデータの特徴や、将来のクエリ用途などに基づいてターゲットテーブルを定義し、作成する方法を主に示しています。テーブルを作成する前に、AWS S3に保存されているソースファイルを確認して、ソースファイル内のデータの特徴を理解することができます。例えば：

- ソースファイルには`datetime`タイプの`Timestamp`列が含まれているため、テーブル作成文にも`datetime`タイプの`Timestamp`列を定義する必要があります。
- ソースファイルのデータには`NULL`値が含まれていないため、テーブル作成文で`NULL`値を許可する列を定義する必要はありません。

- データ型に基づいて、`UserID` 列をソートキーおよびバケットキーとしてテーブル作成ステートメントで定義できます。実際のビジネスシナリオに応じて、`ItemID` などの他の列を定義したり、`UserID` と他の列の組み合わせをソートキーとして定義することもできます。

以下のステートメントでデータベースを作成し、そのデータベースに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

以下のステートメントで手動でテーブルを作成します（テーブルの構造は、AWS S3に保存されているインポートするデータの構造と一致することをお勧めします）：

```SQL
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

テーブル作成後、INSERT INTO SELECT FROM FILES() を使用してテーブルにデータをインポートできます：

```SQL
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **説明**
>
> 上記のコマンド例の `AAA` と `BBB` を実際の有効な Access Key と Secret Key に置き換えてアクセス資格情報として使用します。ここで使用されているデータオブジェクトはすべての合法的な AWS ユーザーに公開されているため、任意の実際の有効な Access Key と Secret Key を入力しても問題ありません。

インポートが完了したら、新しく作成されたテーブルのデータをクエリして、データが正常にインポートされたことを確認できます。例えば：

```SQL
SELECT * from user_behavior_declared LIMIT 3;
```

システムは以下のクエリ結果を返し、データが正常にインポートされたことを示します：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|     58 | 4309692 |    1165503 | pv           | 2017-11-25 14:06:52 |
|     58 |  181489 |    1165503 | pv           | 2017-11-25 14:07:22 |
|     58 | 3722956 |    1165503 | pv           | 2017-11-25 14:09:28 |
+--------+---------+------------+--------------+---------------------+
```

#### インポート進捗の確認

[`information_schema.loads`](../reference/information_schema/loads.md) ビューを通じてインポートジョブの進捗を確認できます。この機能はバージョン 3.1 からサポートされています。例えば：

```SQL
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
```

複数のインポートジョブを提出した場合、`LABEL` を使用して確認したいジョブをフィルタリングできます。例えば：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'insert_e3b882f5-7eb3-11ee-ae77-00163e267b60' \G
*************************** 1. row ***************************
              JOB_ID: 10243
               LABEL: insert_e3b882f5-7eb3-11ee-ae77-00163e267b60
       DATABASE_NAME: mydatabase
               STATE: FINISHED
            PROGRESS: ETL:100%; LOAD:100%
                TYPE: INSERT
            PRIORITY: NORMAL
           SCAN_ROWS: 10000000
       FILTERED_ROWS: 0
     UNSELECTED_ROWS: 0
           SINK_ROWS: 10000000
            ETL_INFO:
           TASK_INFO: resource:N/A; timeout(s):300; max_filter_ratio:0.0
         CREATE_TIME: 2023-11-09 11:56:01
      ETL_START_TIME: 2023-11-09 11:56:01
     ETL_FINISH_TIME: 2023-11-09 11:56:01
     LOAD_START_TIME: 2023-11-09 11:56:01
    LOAD_FINISH_TIME: 2023-11-09 11:56:44
         JOB_DETAILS: {"All backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[10142]},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":311710786,"InternalTableLoadRows":10000000,"ScanBytes":581574034,"ScanRows":10000000,"TaskNumber":1,"Unfinished backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[]}}
           ERROR_MSG: NULL
        TRACKING_URL: NULL
        TRACKING_SQL: NULL
REJECTED_RECORD_PATH: NULL
```

`loads` ビューが提供するフィールドの詳細については、[`information_schema.loads`](../reference/information_schema/loads.md) を参照してください。

> **注記**
>
> INSERT ステートメントは同期コマンドであるため、ジョブが実行中の場合は、別のセッションを開いて INSERT ジョブの実行状況を確認する必要があります。

## Broker Load を使用したインポート

Broker Load は非同期のインポート方法として、AWS S3 との接続を確立し、データを取得して StarRocks に格納します。

現在、Parquet、ORC、CSV の3つのファイル形式をサポートしています。

### Broker Load の利点

- Broker Load はインポートプロセス中に[データ変換](../loading/Etl_in_loading.md)や [UPSERT、DELETE などのデータ変更操作](../loading/Load_to_Primary_Key_tables.md)を実行することをサポートしています。
- Broker Load はバックグラウンドで実行され、クライアントが接続を維持していなくてもインポートジョブが中断されないことを保証します。
- Broker Load のジョブのデフォルトタイムアウトは4時間で、データ量が多く、インポートの実行時間が長いシナリオに適しています。
- Parquet と ORC ファイル形式に加えて、Broker Load は CSV ファイル形式もサポートしています。

### 動作原理

![Broker Load 原理図](../assets/broker_load_how-to-work_zh.png)

1. ユーザーがインポートジョブを作成します。
2. FE がクエリプランを生成し、それを各 BE に分割して割り当てます。
3. 各 BE がデータソースからデータを取得し、StarRocks にインポートします。

### 操作例

StarRocks テーブルを作成し、AWS S3 からサンプルデータセット `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` のデータを取得してインポートジョブを開始し、インポートプロセスと結果が成功したかどうかを確認します。

#### データベースとテーブルの作成

以下のステートメントでデータベースを作成し、そのデータベースに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

以下のステートメントで手動でテーブルを作成します（テーブルの構造は、AWS S3に保存されているインポートするデータの構造と一致することをお勧めします）：

```SQL
CREATE TABLE user_behavior
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### インポートジョブの提出

以下のコマンドを実行して Broker Load ジョブを作成し、サンプルデータセット `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` のデータを `user_behavior` テーブルにインポートします：

```SQL
LOAD LABEL user_behavior
(
    DATA INFILE("s3://starrocks-datasets/user_behavior_ten_million_rows.parquet")
    INTO TABLE user_behavior
    FORMAT AS "parquet"
 )
 WITH BROKER
 (
    "aws.s3.enable_ssl" = "true",
    "aws.s3.use_instance_profile" = "false",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
 )
PROPERTIES
(
    "timeout" = "72000"
);
```

> **説明**
>
> 上記のコマンド例の `AAA` と `BBB` を実際の有効な Access Key と Secret Key に置き換えてアクセス資格情報として使用します。ここで使用されているデータオブジェクトはすべての合法的な AWS ユーザーに公開されているため、任意の実際の有効な Access Key と Secret Key を入力しても問題ありません。

インポートステートメントには4つの部分が含まれています：

- `LABEL`：インポートジョブのラベルで、文字列型で、インポートジョブの状態をクエリするために使用できます。
- `LOAD` 宣言：ソースデータファイルの URI、ソースデータファイルの形式、およびターゲットテーブルの名前などのジョブ記述情報を含みます。
- `BROKER`：データソースへの接続認証情報の設定。
- `PROPERTIES`：タイムアウト時間などのオプションのジョブ属性を指定するために使用されます。

詳細な構文とパラメータの説明については、[BROKER LOAD](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md) を参照してください。

#### インポート進捗の確認

[`information_schema.loads`](../reference/information_schema/loads.md) ビューを通じてインポートジョブの進捗を確認できます。この機能はバージョン 3.1 からサポートされています。

```SQL
SELECT * FROM information_schema.loads;
```

`loads` ビューが提供するフィールドの詳細については、[`information_schema.loads`](../reference/information_schema/loads.md) を参照してください。

複数のインポートジョブを提出した場合、`LABEL` を使用して確認したいジョブをフィルタリングできます。例えば：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'user_behavior';
```

例えば、以下の結果には `user_behavior` インポートジョブに関する2つのレコードがあります：

- 最初のレコードはインポートジョブの状態が `CANCELLED` であることを示しています。`ERROR_MSG` フィールドを通じて、`listPath failed` がジョブのエラーの原因であることが確認できます。
- 2番目のレコードはインポートジョブの状態が `FINISHED` で、ジョブが成功したことを示しています。

```Plaintext
JOB_ID|LABEL                                      |DATABASE_NAME|STATE    |PROGRESS           |TYPE  |PRIORITY|SCAN_ROWS|FILTERED_ROWS|UNSELECTED_ROWS|SINK_ROWS|ETL_INFO|TASK_INFO                                           |CREATE_TIME        |ETL_START_TIME     |ETL_FINISH_TIME    |LOAD_START_TIME    |LOAD_FINISH_TIME   |JOB_DETAILS                                                                                                                                                                                                                                                    |ERROR_MSG                             |TRACKING_URL|TRACKING_SQL|REJECTED_RECORD_PATH|
------+-------------------------------------------+-------------+---------+-------------------+------+--------+---------+-------------+---------------+---------+--------+----------------------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+------------+------------+--------------------+
 10121|user_behavior                              |mydatabase   |CANCELLED|ETL:N/A; LOAD:N/A  |BROKER|NORMAL  |        0|            0|              0|        0|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:59:30|                   |                   |                   |2023-08-10 14:59:34|{"All backends":{},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":0,"InternalTableLoadRows":0,"ScanBytes":0,"ScanRows":0,"TaskNumber":0,"Unfinished backends":{}}                                                                                        |type:ETL_RUN_FAIL; msg:listPath failed|            |            |                    |
 10106|user_behavior                              |mydatabase   |FINISHED |ETL:100%; LOAD:100%|BROKER|NORMAL  | 86953525|            0|              0| 86953525|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:50:15|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:55:10|{"All backends":{"a5fe5e1d-d7d0-4826-ba99-c7348f9a5f2f":[10004]},"FileNumber":1,"FileSize":1225637388,"InternalTableLoadBytes":2710603082,"InternalTableLoadRows":86953525,"ScanBytes":1225637388,"ScanRows":86953525,"TaskNumber":1,"Unfinished backends":{"a5|                                      |            |            |                    |
```

インポートジョブが完了した後、テーブル内でデータをクエリして、データが正常にインポートされたかどうかを確認できます。例えば：

```SQL
SELECT * FROM user_behavior LIMIT 3;
```

システムは以下のクエリ結果を返し、データが正常にインポートされたことを示します：

```Plaintext
+--------+--------+------------+--------------+---------------------+
| UserID | ItemID | CategoryID | BehaviorType | Timestamp           |
+--------+--------+------------+--------------+---------------------+
|     58 | 158350 |    2355072 | pv           | 2017-11-27 13:06:51 |
|     58 | 158590 |    3194735 | pv           | 2017-11-27 02:21:04 |
|     58 | 215073 |    3002561 | pv           | 2017-11-30 10:55:42 |
+--------+--------+------------+--------------+---------------------+
```

## Pipe を通じたインポート

バージョン 3.2 から、StarRocks は Parquet と ORC ファイル形式のみをサポートする Pipe インポート方式を提供しています。

### Pipe の利点

Pipe は、大規模なバッチデータインポートや継続的なデータインポートのシナリオに適しています：

- **大規模なバッチインポートで、エラーのリトライコストを削減します。**

  インポートするデータファイルが多く、データ量が大きい場合。Pipe はファイルの数またはサイズに基づいて、ディレクトリ内のファイルを自動的に分割し、大きなインポートジョブを複数の小さなシリアルインポートタスクに分割します。単一ファイルのデータエラーがインポートジョブ全体の失敗につながることはありません。また、Pipe は各ファイルのインポート状態を記録します。インポートが終了した後、エラーのあるデータファイルを修正し、修正後のデータファイルを再インポートするだけで済みます。これにより、データエラーのリトライコストを削減することができます。

- **継続的なインポートを中断せずに行い、人的操作コストを削減します。**

  新規または変更されたデータファイルを特定のフォルダに書き込み、新規データを StarRocks に継続的にインポートする必要があります。Pipe ベースの継続的なインポートジョブを作成するだけで（ステートメントで `"AUTO_INGEST" = "TRUE"` を指定）、その Pipe は指定されたパス下のデータファイルの変更を継続的に監視し、新規または変更されたデータファイルを自動的に StarRocks のターゲットテーブルにインポートします。

さらに、Pipe はファイルの一意性を判断し、データの重複インポートを避けます。インポートプロセス中に、Pipe はファイル名とファイルに対応する要約値に基づいてデータファイルが重複していないかを判断します。もしファイル名と要約値が同じ Pipe インポートジョブで既に処理されていれば、後続のインポートは処理済みのファイルを自動的にスキップします。なお、AWS S3 などのオブジェクトストレージは `ETag` をファイルの要約値として使用します。

インポートプロセス中のファイル状態は `information_schema.pipe_files` ビューに記録され、このビューを通じて Pipe インポートジョブの各ファイルのインポート状態を確認できます。関連する Pipe ジョブが削除された場合、そのビューの関連記録も同期してクリアされます。

### 動作原理

![Pipe の動作原理](../assets/pipe_data_flow.png)

### Pipe と INSERT+FILES() の違い

Pipe インポート操作は、各データファイルのサイズと行数に基づいて、一つまたは複数のトランザクションに分割され、インポートプロセス中の中間結果がユーザーに表示されます。INSERT+`FILES()` インポート操作は一つの全体トランザクションであり、インポートプロセス中にデータはユーザーに表示されません。

### ファイルインポートの順序

Pipe インポート操作は内部でファイルキューを維持し、キューから対応するファイルをバッチで取り出してインポートします。Pipe はファイルのインポート順序とファイルのアップロード順序を一致させることはできませんので、新しいデータが古いデータよりも早くインポートされる可能性があります。

### 操作例

#### データベースとテーブルの作成

以下のステートメントでデータベースを作成し、そのデータベースに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

以下のステートメントで手動でテーブルを作成します（テーブルの構造は AWS S3 に保存されているインポート対象データの構造と一致することをお勧めします）：

```SQL
CREATE TABLE user_behavior_replica
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### インポートジョブの提出

以下のコマンドを実行して Pipe ジョブを作成し、サンプルデータセット `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` のデータを `user_behavior_replica` テーブルにインポートします：

```SQL
CREATE PIPE user_behavior_replica
PROPERTIES
(
    "AUTO_INGEST" = "TRUE"
)
AS
INSERT INTO user_behavior_replica
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
); 
```

> **説明**
>
> 上記のコマンド例の `AAA` と `BBB` を実際の有効な Access Key と Secret Key に置き換えて、アクセス資格情報として使用します。ここで使用されているデータオブジェクトは、すべての合法的な AWS ユーザーに公開されているため、任意の実際の有効な Access Key と Secret Key を入力しても構いません。

インポートステートメントには4つの部分が含まれています：

- `pipe_name`：Pipe の名前。この名前は Pipe が存在するデータベース内で一意でなければなりません。
- `INSERT_SQL`：INSERT INTO SELECT FROM FILES ステートメント。指定されたソースデータファイルからターゲットテーブルにデータをインポートするために使用されます。
- `PROPERTIES`：Pipe の実行を制御するいくつかのパラメータを設定するために使用されます。例えば、`AUTO_INGEST`、`POLL_INTERVAL`、`BATCH_SIZE`、`BATCH_FILES` などです。形式は `"key" = "value"` です。

詳細な構文とパラメータの説明については、[CREATE PIPE](../sql-reference/sql-statements/data-manipulation/CREATE_PIPE.md) を参照してください。

#### インポート進行状況の確認

- [SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md) を使用して、現在のデータベース内のインポートジョブを確認します。

  ```SQL
  SHOW PIPES;
  ```

  複数のインポートジョブを提出した場合は、`NAME` を使用してどのインポートジョブを確認したいかをフィルタリングできます。例えば：

  ```SQL
  SHOW PIPES WHERE NAME = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR: NULL
   CREATED_TIME: 2023-11-09 15:35:01
  1 row in set (0.01 sec)
  ```

- [`information_schema.pipes`](../reference/information_schema/pipes.md) ビューを使用して、現在のデータベース内のインポートジョブを確認します。

  ```SQL
  SELECT * FROM information_schema.pipes;
  ```

  複数のインポートジョブを提出した場合は、`PIPE_NAME` を使用してどのインポートジョブを確認したいかをフィルタリングできます。例えば：

  ```SQL
  SELECT * FROM information_schema.pipes WHERE pipe_name = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR:
   CREATED_TIME: 9891-01-15 07:51:45
  1 row in set (0.01 sec)
  ```

#### インポートされたファイル情報の確認

[`information_schema.pipe_files`](../reference/information_schema/pipe_files.md) ビューを使用して、インポートされたファイル情報を確認できます。

```SQL
SELECT * FROM information_schema.pipe_files;
```

複数のインポートジョブを提出した場合は、`PIPE_NAME` を使用してどのジョブのファイル情報を確認したいかをフィルタリングできます。例えば：

```SQL
SELECT * FROM information_schema.pipe_files WHERE pipe_name = 'user_behavior_replica' \G
*************************** 1. row ***************************
   DATABASE_NAME: mydatabase
         PIPE_ID: 10217
       PIPE_NAME: user_behavior_replica
       FILE_NAME: s3://starrocks-datasets/user_behavior_ten_million_rows.parquet
    FILE_VERSION: e29daa86b1120fea58ad0d047e671787-8
       FILE_SIZE: 132251298
   LAST_MODIFIED: 2023-11-06 13:25:17
      LOAD_STATE: FINISHED
     STAGED_TIME: 2023-11-09 15:35:02
 START_LOAD_TIME: 2023-11-09 15:35:03
FINISH_LOAD_TIME: 2023-11-09 15:35:42
       ERROR_MSG:
1行がセットされました (0.03秒)
```

#### インポートジョブの管理

Pipeインポートジョブを作成した後、必要に応じてこれらのジョブを変更、一時停止または再開、削除、照会、および再インポートを試みることができます。詳細は [ALTER PIPE](../sql-reference/sql-statements/data-manipulation/ALTER_PIPE.md)、[SUSPEND or RESUME PIPE](../sql-reference/sql-statements/data-manipulation/SUSPEND_or_RESUME_PIPE.md)、[DROP PIPE](../sql-reference/sql-statements/data-manipulation/DROP_PIPE.md)、[SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md)、[RETRY FILE](../sql-reference/sql-statements/data-manipulation/RETRY_FILE.md) を参照してください。
