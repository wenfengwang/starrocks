---
displayed_sidebar: "Chinese"
---

# Import Overview

数据导入 refers to the process of cleaning, transforming, and loading the original data into StarRocks according to business requirements, so that fast unified data analysis can be performed in the StarRocks system.

StarRocks achieves data import through import jobs. Each import job has a label, which is specified by the user or generated by the system, used to identify the import job. Each label is unique within a database and can only be used for a successful import job. After a successful import job, its label cannot be used to submit other import jobs. Only the labels of failed import jobs can be used to submit other import jobs. This mechanism ensures that data corresponding to any label is imported at most once, that is, it achieves "at most once (At-Most-Once)" semantics.

StarRocks provides atomicity guarantee for all import methods, that is, all valid data within the same import job will either all take effect or none will take effect, and there will be no situation where only part of the data is imported. Here, valid data does not include data filtered out due to data quality issues such as type conversion errors.

StarRocks provides two types of access protocols for submitting import jobs: MySQL protocol and HTTP protocol. Different import methods support different access protocols, please refer to the "[Import Methods](../loading/Loading_intro.md#导入方式)" section of this document for specific details.

> **Note**
>
> Import operations require INSERT permission for the target table. If your user account does not have INSERT permission, please refer to [GRANT](../sql-reference/sql-statements/account-management/GRANT.md) to grant permission to the user.

## Supported Data Types

StarRocks supports the import of all data types. There may be some limitations for the import of individual data types, please refer to [Data Types](../sql-reference/sql-statements/data-types/BIGINT.md) for specific details.

## Import Modes

StarRocks supports two import modes: synchronous import and asynchronous import.

> **Note**
>
> If external programs access the import of StarRocks, you need to determine which import mode to use first, and then determine the access logic.

### Synchronous Import

Synchronous import means that after you create an import job, StarRocks executes the job synchronously and returns the import result after the job is executed. You can judge whether the import job is successful based on the returned import result.

The import methods that support synchronous mode are Stream Load and INSERT.

The user operation process is as follows:

1. Create an import job.

2. View the import result returned by StarRocks.

3. Judge the import result. If the import result is a failure, you can retry the import job.

### Asynchronous Import

Asynchronous import means that after you create an import job, StarRocks directly returns the job creation result.

- If the import job is successfully created, StarRocks will execute the import job asynchronously. However, the successful creation of the job does not mean that the data import has been successful. You need to check the status of the import job through statements or commands and judge whether the data import is successful based on the status of the import job.
- If the import job creation fails, you can judge whether a retry is needed based on the failure information.

The import methods that support asynchronous mode are Broker Load, Routine Load, and Spark Load.

The user operation process is as follows:

1. Create an import job.

2. Judge whether the job is successfully created based on the job creation result returned by StarRocks.

   - If the job is successfully created, proceed to step 3.
   - If the job creation fails, you can return to step 1 and retry the import job.

3. Poll the status of the import job until the status becomes **FINISHED** or **CANCELLED**.

The execution process of Broker Load and Spark Load import jobs mainly consists of 5 stages, as shown in the following figure.

![Broker Load and Spark Load Process Diagram](../assets/4.1-1.png)

The description of each stage is as follows:

1. **PENDING**

   This stage refers to waiting for the FE to schedule the execution after submitting the import job.

2. **ETL**

   This stage performs preprocessing of the data, including cleaning, partitioning, sorting, aggregation, and so on.

   > **Note**
   >
   > If it is a Broker Load job, this stage will be completed directly.

3. **LOADING**

   In this stage, the data is first cleaned and transformed, and then sent to BE for processing. When all the data is imported, it enters the waiting for effectiveness process. At this time, the status of the import job is still **LOADING**.

4. **FINISHED**

   After all the data involved in the import job takes effect, the status of the job becomes **FINISHED**, at this time, the imported data can all be queried. **FINISHED** is the final status of the import job.

5. **CANCELLED**

   Before the status of the import job becomes **FINISHED**, you can cancel the job at any time. In addition, if an error occurs during the import, the StarRocks system will also automatically cancel the import job. After the job is cancelled, it enters the **CANCELLED** status. **CANCELLED** is also one of the final statuses of the import job.

The execution process of Routine Load import jobs is described as follows:

1. The user submits an import job to the FE through a client supporting the MySQL protocol.

2. The FE splits the import job into several tasks, and each task is responsible for importing data for several partitions.

3. The FE assigns various tasks to designated BEs for execution.

4. After the BE completes the assigned tasks, it reports to the FE.

5. Based on the reporting results, the FE continues to generate subsequent tasks, or retries failed tasks, or pauses task scheduling.

## Import Methods

StarRocks provides multiple import methods including [Stream Load](../sql-reference/sql-statements/data-manipulation/STREAM_LOAD.md), [Broker Load](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md), [Routine Load](../sql-reference/sql-statements/data-manipulation/CREATE_ROUTINE_LOAD.md), [Spark Load](../sql-reference/sql-statements/data-manipulation/SPARK_LOAD.md), and [INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md), to meet your data import requirements in different business scenarios.

| Import Method     | Data Source                                                                                   | Business Scenario                                                                                             | Data Volume (Per Job) | Data Format                                         | Synchronous Mode | Protocol |
| ----------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- | -------------------- | --------------------------------------------------- | ----------------- | -------- |
| Stream Load       |<ul><li>Local file</li><li>Streaming data</li></ul>                                             | Import local files via HTTP protocol, or import data streams through programs.                                 | Within 10 GB        |<ul><li>CSV</li><li>JSON</li></ul>                    | Synchronous      | HTTP     |
| Broker Load       |<ul><li>HDFS</li><li>Amazon S3</li><li>Google GCS</li><li>Microsoft Azure Storage</li><li>Aliyun OSS</li><li>Tencent Cloud COS</li><li>Huawei Cloud OBS</li><li>Other S3-compatible object storage (such as MinIO)</li></ul> | Import data from HDFS or external cloud storage systems.                                                          | Tens to hundreds of GB |<ul><li>CSV</li><li>Parquet</li><li>ORC</li></ul>| Asynchronous      | MySQL |
| Routine Load       | Apache Kafka®                                                                                 | 从 Kafka 实时地导入数据流。                                                                             | 微批导入 MB 到 GB 级 |<ul><li>CSV</li><li>JSON</li><li>Avro（3.0.1 版本之后支持）</li></ul>          | 异步     | MySQL |
| Spark Load         | <ul><li>HDFS</li><li>Hive</li></ul>                                                            | <ul><li>通过 Apache Spark™ 集群初次从 HDFS 或 Hive 迁移导入大量数据。</li><li>需要做全局数据字典来精确去重。</li></ul> | 数十 GB 到 TB级别    |<ul><li>CSV</li><li>ORC（2.0 版本之后支持）</li><li>Parquet（2.0 版本之后支持）</li></ul>       | 异步     | MySQL |
| INSERT INTO SELECT |<ul><li>StarRocks 表</li><li>外部表</li><li>AWS S3</li><li>HDFS</li></ul>**注意**<br />从 AWS S3 或 HDFS 导入数据时，只支持导入 Parquet 和 ORC 格式的数据。                                                    |<ul><li>外表导入。</li><li>StarRocks 数据表之间的数据导入。</li></ul>                                              | 跟内存相关           | StarRocks 表                                     | 同步        | MySQL |
| INSERT INTO VALUES |<ul><li>程序</li><li>ETL 工具</li></ul>                                                          |<ul><li>单条批量小数据量插入。</li><li>通过 JDBC 等接口导入。</li></ul>                                             | 简单测试用           | SQL                                              | 同步        | MySQL |

您可以根据业务场景、数据量、数据源、数据格式和导入频次等来选择合适的导入方式。另外，在选择导入方式时，可以注意以下几点：

- 从 Kafka 导入数据时，推荐使用 [Routine Load](../loading/RoutineLoad.md) 实现导入。但是，如果导入过程中有复杂的多表关联和 ETL 预处理，建议先使用 Apache Flink® 从 Kafka 读取数据并对数据进行处理，然后再通过 StarRocks 提供的标准插件 [flink-connector-starrocks](../loading/Flink-connector-starrocks.md) 把处理后的数据导入到 StarRocks 中。

- 从 Hive、Iceberg、Hudi、Delta Lake 导入数据时，推荐创建 [Hive catalog](../data_source/catalog/hive_catalog.md)、[Iceberg catalog](../data_source/catalog/iceberg_catalog.md)、[Hudi Catalog](../data_source/catalog/hudi_catalog.md)、[Delta Lake Catalog](../data_source/catalog/deltalake_catalog.md)，然后使用 [INSERT](../loading/InsertInto.md) 实现导入。

- 从另外一个 StarRocks 集群或从 Elasticsearch 导入数据时，推荐创建 [StarRocks 外部表](../data_source/External_table.md#starrocks-外部表)或 [Elasticsearch 外部表](../data_source/External_table.md#deprecated-elasticsearch-外部表)，然后使用 [INSERT](../loading/InsertInto.md) 实现导入。或者，您也可以通过 [DataX](../loading/DataX-starrocks-writer.md) 实现导入。

  > **注意**
  >
  > StarRocks 外表只支持数据写入，不支持数据读取。

- 从 MySQL 导入数据时，推荐创建 [MySQL 外部表](../data_source/External_table.md#deprecated-mysql-外部表)、然后使用 [INSERT](../loading/InsertInto.md) 实现导入。或者，您也可以通过 [DataX](../loading/DataX-starrocks-writer.md) 实现导入。如果要导入实时数据，建议您参考 [从 MySQL 实时同步](../loading/Flink_cdc_load.md) 实现导入。

- 从 Oracle、PostgreSQL 或 SQL Server 等数据源导入数据时，推荐创建 [JDBC 外部表](../data_source/External_table.md#更多数据库jdbc的外部表)、然后使用 [INSERT](../loading/InsertInto.md) 实现导入。或者，您也可以通过 [DataX](../loading/DataX-starrocks-writer.md) 实现导入。

下图详细展示了在各种数据源场景下，应该选择哪一种导入方式。

![数据源与导入方式关系图](../assets/4.1-3.png)

## 内存限制

您可以通过设置参数来限制单个导入作业的内存使用，以防止导入作业占用过多内存，特别是在导入并发较高的情况下。同时，您也需要注意避免设置过小的内存使用上限，因为内存使用上限过小，导入过程中可能会因为内存使用量达到上限而频繁地将内存中的数据刷出到磁盘，进而可能影响导入效率。建议您根据具体的业务场景要求，合理地设置内存使用上限。

不同的导入方式限制内存的方式略有不同，具体请参见 [Stream Load](../sql-reference/sql-statements/data-manipulation/STREAM_LOAD.md)、[Broker Load](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)、[Routine Load](../sql-reference/sql-statements/data-manipulation/CREATE_ROUTINE_LOAD.md)、[Spark Load](../sql-reference/sql-statements/data-manipulation/SPARK_LOAD.md) 和 [INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md)。需要注意的是，一个导入作业通常都会分布在多个 BE 上执行，这些内存参数限制的是一个导入作业在单个 BE 上的内存使用，而不是在整个集群上的内存使用总和。

您还可以通过设置一些参数来限制在单个 BE 上运行的所有导入作业的总的内存使用上限。可参考本文“[系统配置](../loading/Loading_intro.md#系统配置)”章节。

## 使用说明

### 导入自动赋值

导入数据时，您可以指定不导入数据文件中某个字段的数据，这种情况下：

- 如果您在创建 StarRocks 表时使用 `DEFAULT` 关键字给该字段对应的目标列指定了默认值，则 StarRocks 在导入时该行数据时会自动往该列填充 `DEFAULT` 中指定的默认值。

  [Stream Load](../sql-reference/sql-statements/data-manipulation/STREAM_LOAD.md)、[Broker Load](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)、[Routine Load](../sql-reference/sql-statements/data-manipulation/CREATE_ROUTINE_LOAD.md) 和 [INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md) 四种导入方式当前支持 `DEFAULT current_timestamp`、`DEFAULT <默认值>` 和 `DEFAULT (<表达式>)`。[Spark Load](../sql-reference/sql-statements/data-manipulation/SPARK_LOAD.md) 导入方式当前仅支持 `DEFAULT current_timestamp` 和 `DEFAULT <默认值>`，不支持 `DEFAULT (<表达式>)`。

  > **说明**
  >
  > 目前 `DEFAULT (<表达式>)` 仅支持 `uuid()` 和 `uuid_numeric()` 函数。

- 如果您在创建 StarRocks 表时没有使用 `DEFAULT` 关键字给该字段对应的目标列指定默认值，则 StarRocks 在导入该行数据时会自动往该列填充 `NULL` 值。

  > **说明**
  >
  > 如果该列在建表时定义该列为 `NOT NULL`，则导入会报错，作业失败。

对于 [Stream Load](../sql-reference/sql-statements/data-manipulation/STREAM_LOAD.md)、[Broker Load](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)、[Routine Load](../sql-reference/sql-statements/data-manipulation/CREATE_ROUTINE_LOAD.md) 和 [Spark Load](../sql-reference/sql-statements/data-manipulation/SPARK_LOAD.md)，您还可以在指定待导入列的参数里通过函数来给该列指定要填充的值。

有关 `NOT NULL` 和 `DEFAULT` 的用法，请参见[CREATE TABLE](../sql-reference/sql-statements/data-definition/CREATE_TABLE.md)。

### 设置数据导入安全等级

如果您的 StarRocks 集群有多数据副本，您可以根据业务需求为 Table 设置不同导入数据安全等级，即设置需要多少数据副本导入成功后 StarRocks 可返回导入成功。您可在[CREATE TABLE](../sql-reference/sql-statements/data-definition/CREATE_TABLE.md)时通过增加属性（PROPERTIES）`write_quorum`指定导入数据安全等级，或通过[ALTER TABLE](../sql-reference/sql-statements/data-definition/ALTER_TABLE.md)语句为已有 Table添加该属性。该属性从 2.5 版本开始支持。

## 系统配置

本节解释对所有导入方式均适用的参数配置。

### FE 配置

您可以通过修改每个 FE 的配置文件**fe.conf**来设置如下参数：

- `max_load_timeout_second` 和 `min_load_timeout_second`

  设置导入超时时间的最大、最小值，单位均为秒。默认的最大超时时间为 3 天，默认的最小超时时间为 1 秒。自定义的导入超时时间不能超过这个最大、最小值范围。该参数配置适用于所有模式的导入作业。

- `desired_max_waiting_jobs`

  等待队列可以容纳的导入作业的最大个数，默认值为 1024 (2.4 及之前版本默认值为 100；2.5 及以后版本默认值变为 1024)。如果 FE 中处于**PENDING**状态的导入作业数目达到最大个数限制时，FE 会拒绝新的导入请求。该参数配置仅对异步执行的导入有效。

- `max_running_txn_num_per_db`

  StarRocks 集群每个数据库中正在进行的导入事务的最大个数（一个导入作业可能包含多个事务），默认值为 1000 （自 3.1 版本起，默认值由 100 变为 1000。）。当数据库中正在运行的导入事务达到最大个数限制时，后续提交的导入作业不会执行。如果是同步的导入作业，作业会被拒绝；如果是异步的导入作业，作业会在队列中等待。

  > **说明**
  >
  > 所有模式的作业均包含在内、统一计数。

- `label_keep_max_second`

  已经完成、且处于**FINISHED**或**CANCELLED**状态的导入作业记录在 StarRocks 系统的保留时长，默认值为 3 天。该参数配置适用于所有模式的导入作业。

### BE 配置

您可以通过修改每个 BE 的配置文件**be.conf**来设置如下参数：

- `write_buffer_size`

  BE 上内存块的大小阈值，默认阈值为 100 MB。导入的数据在 BE 上会先写入一个内存块，当内存块的大小达到这个阈值以后才会写回磁盘。如果阈值过小，可能会导致 BE 上存在大量的小文件，影响查询的性能，这时候可以适当提高这个阈值来减少文件数量。如果阈值过大，可能会导致远程过程调用（Remote Procedure Call，简称 RPC）超时，这时候可以适当地调整该参数的取值。

- `streaming_load_rpc_max_alive_time_sec`

  指定了 Writer 进程的等待超时时间，默认为 600 秒。在导入过程中，StarRocks 会为每个 Tablet 开启一个 Writer 进程，用于接收和写入数据。如果在参数指定时间内 Writer 进程没有收到任何数据，StarRocks 系统会自动销毁这个 Writer 进程。当系统处理速度较慢时，Writer 进程可能长时间接收不到下一批次数据，导致上报 "TabletWriter add batch with unknown id" 错误。这时候可适当调大这个参数的取值。

- `load_process_max_memory_limit_bytes` 和 `load_process_max_memory_limit_percent`

  用于导入的最大内存使用量和最大内存使用百分比，用来限制单个 BE 上所有导入作业的内存总和的使用上限。StarRocks 系统会在两个参数中取较小者，作为最终的使用上限。

  - `load_process_max_memory_limit_bytes`：指定 BE 上最大内存使用量，默认为 100 GB。
  - `load_process_max_memory_limit_percent`：指定 BE 上最大内存使用百分比，默认为 30%。该参数与`mem_limit`参数不同。`mem_limit`参数指定的是 BE 进程内存上限，默认硬上限为 BE 所在机器内存的 90%，软上限为 BE 所在机器内存的 90% x 90%。

    假设 BE 所在机器物理内存大小为 M，则用于导入的内存上限为：`M x 90% x 90% x 30%`。

### 会话变量

您可以设置如下[会话变量](../reference/System_variable.md)：

- `query_timeout`

  用于设置查询超时时间。单位：秒。取值范围：`1` ~ `259200`。默认值：`300`，相当于 5 分钟。该变量会作用于当前连接中所有的查询语句，以及 INSERT 语句。

## 常见问题

请参见[导入常见问题](../faq/loading/Loading_faq.md)。