---
displayed_sidebar: "Chinese"
---

# Import from AWS S3

import LoadMethodIntro from '../assets/commonMarkdown/loadMethodIntro.md'

import InsertPrivNote from '../assets/commonMarkdown/insertPrivNote.md'

StarRocks supports importing data from AWS S3 in the following ways:

<LoadMethodIntro />

## Preparation

### Prepare the Data Source

Make sure the data to be imported is saved in the S3 bucket. It is recommended to save the data in the same region as the StarRocks cluster, which can reduce data transmission costs.

In this article, we provide a sample dataset `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` open to all valid AWS users. You just need to configure real and valid security credentials to access the dataset.

### Check Permissions

<InsertPrivNote />

### Obtain Resource Access Configuration

The examples in this article use the authentication method based on IAM User. To ensure that you can successfully access the data stored in AWS S3, it is recommended to create an IAM User and configure the [IAM policy](../reference/aws_iam_policies.md) based on the preparations introduced in "[Authenticate to AWS Resources Using IAM User](../integrations/authenticate_to_aws_resources.md)".

In general, if you choose to use the authentication method based on IAM User, you need to obtain the following AWS resource information in advance:

- The S3 storage bucket where the data is located
- S3 object key (or "object name") (only required when accessing a specific data object in the S3 bucket. Note that if the data object to be accessed is stored in a subfolder, its name can include a prefix.)
- AWS region where the S3 bucket is located
- Access Key and Secret Key as access credentials

For other authentication methods supported by StarRocks, see [Configure AWS Authentication Information](../integrations/authenticate_to_aws_resources.md).

## Import via INSERT+FILES()

This feature is supported from version 3.1 onwards. Currently, it only supports the Parquet and ORC file formats.

### Advantages of INSERT+FILES()

`FILES()` reads data based on the given data path and other parameters, and automatically infers the table structure based on the data file format, column information, etc., and finally returns the data in the form of data rows.

With `FILES()`, you can:

- Use [SELECT](../sql-reference/sql-statements/data-manipulation/SELECT.md) statements to query data directly from AWS S3.
- Automatically create a table and import data through the [CREATE TABLE AS SELECT](../sql-reference/sql-statements/data-definition/CREATE_TABLE_AS_SELECT.md) (referred to as CTAS) statement.
- Manually create a table and then import data through [INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md).

### Operation Example

#### Directly Query Data via SELECT

You can directly query the data in AWS S3 using SELECT+`FILES()`, which allows you to have an overall understanding of the data to be imported before creating the table, including the advantages of:

- You do not need to store the data to view it.
- You can view the maximum and minimum values of the data and determine which data types to use.
- You can check if the data contains `NULL` values.

For example, query the data in the sample dataset `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`:

```SQL
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
)
LIMIT 3;
```

> **Note**
>
> Replace the `AAA` and `BBB` in the example command with real and valid Access Key and Secret Key as access credentials. Since the data object used here is open to all valid AWS users, you can input any real and valid Access Key and Secret Key.

The system returns the following query result:

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
| 543711 |  829192 |    2355072 | pv           | 2017-11-27 08:22:37 |
| 543711 | 2056618 |    3645362 | pv           | 2017-11-27 10:16:46 |
| 543711 | 1165492 |    3645362 | pv           | 2017-11-27 10:17:00 |
+--------+---------+------------+--------------+---------------------
```

> **Note**
>
> The column names in the above query result are defined in the source Parquet file.

#### Automatically Create Table and Import Data via CTAS

This example is a continuation of the previous one. In this example, by nesting the SELECT query from the previous example in the CREATE TABLE AS SELECT (CTAS) statement, StarRocks can automatically infer the table structure, create the table, and import the data into the newly created table. The Parquet format file comes with column names and data types, so you do not need to specify the column names or data types.

> **Note**
>
> When using the table structure inference feature, the CREATE TABLE statement does not support setting the replication number, so you need to set the replication number before creating the table. For example, you can set the replication number to `1` with the following command:
>
> ```SQL
> ADMIN SET FRONTEND CONFIG ('default_replication_num' = "1");
> ```

Create a database and switch to this database with the following statement:

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

Automatically create a table and import the data from the sample dataset `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` into the newly created table via CTAS:

```SQL
CREATE TABLE user_behavior_inferred AS
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **Note**
>
> Replace the `AAA` and `BBB` in the example command with real and valid Access Key and Secret Key as access credentials. Since the data object used here is open to all valid AWS users, you can input any real and valid Access Key and Secret Key.

After creating the table, you can use [DESCRIBE](../sql-reference/sql-statements/Utility/DESCRIBE.md) to view the table structure of the newly created table:

```SQL
DESCRIBE user_behavior_inferred;
```

The system returns the following query result:

```Plaintext
+--------------+------------------+------+-------+---------+-------+
| Field        | Type             | Null | Key   | Default | Extra |
+--------------+------------------+------+-------+---------+-------+
| UserID       | bigint           | YES  | true  | NULL    |       |
| ItemID       | bigint           | YES  | true  | NULL    |       |
| CategoryID   | bigint           | YES  | true  | NULL    |       |
| BehaviorType | varchar(1048576) | YES  | false | NULL    |       |
| Timestamp    | varchar(1048576) | YES  | false | NULL    |       |
+--------------+------------------+------+-------+---------+-------+
```

Compare the table structure inferred by the system with the table structure manually created in terms of the following aspects:

- Data types
- 是否允许 `NULL` 值
- 定义为键的字段

在生产环境中，建议您手动创建表、指定表结构，以更好地控制目标表的表结构并实现更高的查询性能。

您可以查询新建表中的数据，验证数据已成功导入。例如：

```SQL
SELECT * from user_behavior_inferred LIMIT 3;
```

系统返回如下查询结果，表明数据已成功导入：

```Plaintext
+--------+--------+------------+--------------+---------------------+
| UserID | ItemID | CategoryID | BehaviorType | Timestamp           |
+--------+--------+------------+--------------+---------------------+
|     58 | 158350 |    2355072 | pv           | 2017-11-27 13:06:51 |
|     58 | 158590 |    3194735 | pv           | 2017-11-27 02:21:04 |
|     58 | 215073 |    3002561 | pv           | 2017-11-30 10:55:42 |
+--------+--------+------------+--------------+---------------------+
```

#### 手动建表并通过 INSERT 导入数据

在实际业务场景中，您可能需要自定义目标表的表结构，包括：

- 各列的数据类型和默认值、以及是否允许 `NULL` 值
- 定义哪些列作为键、以及这些列的数据类型
- 数据分区分桶

> **说明**
>
> 要实现高效的表结构设计，您需要深度了解表中数据的用途、以及表中各列的内容。本文不对表设计做过多赘述，有关表设计的详细信息，参见[表设计](../table_design/StarRocks_table_design.md)。

该示例主要演示如何根据源 Parquet 格式文件中数据的特点、以及目标表未来的查询用途等对目标表进行定义和创建。在创建表之前，您可以先查看一下保存在 AWS S3 中的源文件，从而了解源文件中数据的特点，例如：

- 源文件中包含一个数据类型为 `datetime` 的 `Timestamp` 列，因此建表语句中也应该定义这样一个数据类型为 `datetime` 的 `Timestamp` 列。
- 源文件中的数据中没有 `NULL` 值，因此建表语句中也不需要定义任何列为允许 `NULL` 值。
- 根据查询到的数据类型，可以在建表语句中定义 `UserID` 列为排序键和分桶键。根据实际业务场景需要，您还可以定义其他列比如 `ItemID` 或者定义 `UserID` 与其他列的组合作为排序键。

通过如下语句创建数据库、并切换至该数据库：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

通过如下语句手动创建表（建议表结构与您在 AWS S3 存储的待导入数据结构一致）：

```SQL
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

建表完成后，您可以通过 INSERT INTO SELECT FROM FILES() 向表内导入数据：

```SQL
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **说明**
>
> 把上面命令示例中的 `AAA` 和 `BBB` 替换成真实有效的 Access Key 和 Secret Key 作为访问凭证。由于这里使用的数据对象对所有合法的 AWS 用户开放，因此您填入任何真实有效的 Access Key 和 Secret Key 都可以。

导入完成后，您可以查询新建表中的数据，验证数据已成功导入。例如：

```SQL
SELECT * from user_behavior_declared LIMIT 3;
```

系统返回如下查询结果，表明数据已成功导入：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|     58 | 4309692 |    1165503 | pv           | 2017-11-25 14:06:52 |
|     58 |  181489 |    1165503 | pv           | 2017-11-25 14:07:22 |
|     58 | 3722956 |    1165503 | pv           | 2017-11-25 14:09:28 |
+--------+---------+------------+--------------+---------------------+
```

#### 查看导入进度

通过 [`information_schema.loads`](../reference/information_schema/loads.md) 视图查看导入作业的进度。该功能自 3.1 版本起支持。例如：

```SQL
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
```

如果您提交了多个导入作业，您可以通过 `LABEL` 过滤出想要查看的作业。例如：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'insert_e3b882f5-7eb3-11ee-ae77-00163e267b60' \G
*************************** 1. row ***************************
              JOB_ID: 10243
               LABEL: insert_e3b882f5-7eb3-11ee-ae77-00163e267b60
       DATABASE_NAME: mydatabase
               STATE: FINISHED
            PROGRESS: ETL:100%; LOAD:100%
                TYPE: INSERT
            PRIORITY: NORMAL
           SCAN_ROWS: 10000000
       FILTERED_ROWS: 0
     UNSELECTED_ROWS: 0
           SINK_ROWS: 10000000
            ETL_INFO:
           TASK_INFO: resource:N/A; timeout(s):300; max_filter_ratio:0.0
         CREATE_TIME: 2023-11-09 11:56:01
      ETL_START_TIME: 2023-11-09 11:56:01
     ETL_FINISH_TIME: 2023-11-09 11:56:01
     LOAD_START_TIME: 2023-11-09 11:56:01
    LOAD_FINISH_TIME: 2023-11-09 11:56:44
         JOB_DETAILS: {"All backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[10142]},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":311710786,"InternalTableLoadRows":10000000,"ScanBytes":581574034,"ScanRows":10000000,"TaskNumber":1,"Unfinished backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[]}}
           ERROR_MSG: NULL
        TRACKING_URL: NULL
        TRACKING_SQL: NULL
REJECTED_RECORD_PATH: NULL
```

有关 `loads` 视图提供的字段详情，参见 [`information_schema.loads`](../reference/information_schema/loads.md)。

> **NOTE**
>
> 由于 INSERT 语句是一个同步命令，因此，如果作业还在运行当中，您需要打开另一个会话来查看 INSERT 作业的执行情况。

## 通过 Broker Load 导入

作为一种异步的导入方式，Broker Load 负责建立与 AWS S3 的连接、拉取数据、并将数据存储到 StarRocks 中。

当前支持 Parquet、ORC、及 CSV 三种文件格式。

### Broker Load 优势

- Broker Load supports [data transformation](../loading/Etl_in_loading.md) and data change operations such as UPSERT and DELETE during the import process, as well as [loading to Primary Key tables](../loading/Load_to_Primary_Key_tables.md).
- Broker Load runs in the background, and the client does not need to maintain a connection to ensure that the import job is not interrupted.
- The default timeout for Broker Load jobs is 4 hours, suitable for scenarios where large amounts of data need to be imported and the import process takes a long time.
- In addition to Parquet and ORC file formats, Broker Load also supports CSV file format.

### Working Principle

![Broker Load Working Principle](../assets/broker_load_how-to-work_zh.png)

1. Users create import jobs.
2. FE generates a query plan, then splits the query plan and assigns it to various BEs for execution.
3. Various BEs pull data from the data source and import the data into StarRocks.

### Operation Example

Create a StarRocks table, start an import job to pull sample data set `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` from AWS S3, and then verify the success of the import process and results.

#### Create Database and Table

Create a database and switch to that database using the following statement:

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

Manually create a table using the following statement (it is recommended that the table structure matches the structure of the data to be imported stored in AWS S3):

```SQL
CREATE TABLE user_behavior
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### Submit Import Job

Execute the following command to create a Broker Load job to import data from the sample data set `s3://starrocks-s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` into the table `user_behavior`:

```SQL
LOAD LABEL user_behavior
(
    DATA INFILE("s3://starrocks-datasets/user_behavior_ten_million_rows.parquet")
    INTO TABLE user_behavior
    FORMAT AS "parquet"
 )
 WITH BROKER
 (
    "aws.s3.enable_ssl" = "true",
    "aws.s3.use_instance_profile" = "false",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
 )
PROPERTIES
(
    "timeout" = "72000"
);
```

> **Note**
>
> Replace `AAA` and `BBB` in the example command above with real and valid Access Key and Secret Key as access credentials. As the data object used here is open to all valid AWS users, you can use any real and valid Access Key and Secret Key.

The import statement includes four parts:

- `LABEL`: the label of the import job, a string type that can be used to query the status of the import job.
- `LOAD` statement: includes job description information such as the URI of the source data file, the format of the source data file, and the name of the target table.
- `BROKER`: authentication information configuration for connecting to the data source.
- `PROPERTIES`: used to specify optional job properties such as timeout.

For detailed syntax and parameter explanations, see [BROKER LOAD](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md).

#### View Import Progress

Use the [`information_schema.loads`](../reference/information_schema/loads.md) view to check the progress of the import job. This feature is supported from version 3.1 onwards.

```SQL
SELECT * FROM information_schema.loads;
```

For details of the fields provided by the `loads` view, see [`information_schema.loads`](../reference/information_schema/loads.md).

If you have submitted multiple import jobs, you can filter out the job you want to view by using `LABEL`. For example:

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'user_behavior';
```

For example, in the returned result below, there are two records related to the import job `user_behavior`:

- The first record shows that the import job is in the `CANCELLED` state. The `ERROR_MSG` field in the record indicates that the reason for the job failure is `listPath failed`.
- The second record shows that the import job is in the `FINISHED` state, indicating a successful job.

```Plaintext
JOB_ID|LABEL                                      |DATABASE_NAME|STATE    |PROGRESS           |TYPE  |PRIORITY|SCAN_ROWS|FILTERED_ROWS|UNSELECTED_ROWS|SINK_ROWS|ETL_INFO|TASK_INFO                                           |CREATE_TIME        |ETL_START_TIME     |ETL_FINISH_TIME    |LOAD_START_TIME    |LOAD_FINISH_TIME   |JOB_DETAILS                                                                                                                                                                                                                                                    |ERROR_MSG                             |TRACKING_URL|TRACKING_SQL|REJECTED_RECORD_PATH|
------+-------------------------------------------+-------------+---------+-------------------+------+--------+---------+-------------+---------------+---------+--------+----------------------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+------------+------------+--------------------+
 10121|user_behavior                              |mydatabase   |CANCELLED|ETL:N/A; LOAD:N/A  |BROKER|NORMAL  |        0|            0|              0|        0|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:59:30|                   |                   |                   |2023-08-10 14:59:34|{"All backends":{},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":0,"InternalTableLoadRows":0,"ScanBytes":0,"ScanRows":0,"TaskNumber":0,"Unfinished backends":{}}                                                                                        |type:ETL_RUN_FAIL; msg:listPath failed|            |            |                    |
 10106|user_behavior                              |mydatabase   |FINISHED |ETL:100%; LOAD:100%|BROKER|NORMAL  | 86953525|            0|              0| 86953525|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:50:15|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:55:10|{"All backends":{"a5fe5e1d-d7d0-4826-ba99-c7348f9a5f2f":[10004]},"FileNumber":1,"FileSize":1225637388,"InternalTableLoadBytes":2710603082,"InternalTableLoadRows":86953525,"ScanBytes":1225637388,"ScanRows":86953525,"TaskNumber":1,"Unfinished backends":{"a5|                                      |            |            |                    |
```

After the import job is completed, you can query the table to verify whether the data has been successfully imported. For example:

```SQL
SELECT * from user_behavior LIMIT 3;
```

The system returns the following query result, indicating that the data has been successfully imported:

```Plaintext
+--------+--------+------------+--------------+---------------------+
| UserID | ItemID | CategoryID | BehaviorType | Timestamp           |
+--------+--------+------------+--------------+---------------------+
|     58 | 158350 |    2355072 | pv           | 2017-11-27 13:06:51 |
|     58 | 158590 |    3194735 | pv           | 2017-11-27 02:21:04 |
|     58 | 215073 |    3002561 | pv           | 2017-11-30 10:55:42 |
+--------+--------+------------+--------------+---------------------+
```

## Importing via Pipe

Starting from version 3.2, StarRocks provides the Pipe import method, which currently only supports the Parquet and ORC file formats.

### Advantages of Pipe

Pipe is suitable for scenarios where large-scale batch data import and continuous data import are required:

- **大规模分批导入，降低出错重试成本。**

  需要导入的数据文件较多、数据量大。Pipe 会按文件数量或大小，自动对目录下的文件进行拆分，将一个大的导入作业拆分成多个较小的串行的导入任务。单个文件的数据错误不会导致整个导入作业的失败。另外，Pipe 会记录每个文件的导入状态。当导入结束后，您可以修复出错的数据文件，然后重新导入修正后的数据文件即可。这有助于降低数据出错重试的代价。

- **不间断持续导入，减少人力操作成本。**

  需要将新增或变化的数据文件写入到某个文件夹下，并且新增的数据需要持续地导入到 StarRocks 中。您只需要创建一个基于 Pipe 的持续导入作业（在语句中指定 `"AUTO_INGEST" = "TRUE"`），该 Pipe 会持续监控该作业中指定的路径下的数据文件变化，将新增或有变动的数据文件自动导入到 StarRocks 目标表中。

此外，Pipe 还支持文件唯一性判断，避免重复数据导入。在导入过程中，Pipe 会根据文件名和文件对应的摘要值判断数据文件是否重复。如果文件名和文件摘要值在同一个 Pipe 导入作业中已经处理过，后续导入会自动跳过已经处理过的文件。注意，AWS S3 等对象存储使用 `ETag` 作为文件摘要

导入过程中的文件状态会记录到 `information_schema.pipe_files` 视图下，您可以通过该视图查看 Pipe 导入作业下各文件的导入状态。如果该视图关联的 Pipe 作业被删除，那么该视图下相关的记录也会同步清理。

### 工作原理

![Pipe 工作原理](../assets/pipe_data_flow.png)

### Pipe 与 INSERT+FILES() 的区别

Pipe 导入操作会根据每个数据文件的大小和包含的行数，分割成一个或多个事务，导入过程中的中间结果对用户可见。INSERT+`FILES()` 导入操作是一个整体事务，导入过程中数据对用户不可见。

### 文件导入顺序

Pipe 导入操作会在内部维护一个文件队列，分批次从队列中取出对应文件进行导入。Pipe 并不能保证文件的导入顺序和文件上传顺序一致，因此可能会出现新的数据早于老的数据导入。

### 操作示例

#### 建库建表

通过如下语句创建数据库、并切换至该数据库：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

通过如下语句手动创建表（建议表结构与您在 AWS S3 存储的待导入数据结构一致）：

```SQL
CREATE TABLE user_behavior_replica
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### 提交导入作业

执行如下命令创建 Pipe 作业，把样例数据集 `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` 中的数据导入到表 `user_behavior_replica` 中：

```SQL
CREATE PIPE user_behavior_replica
PROPERTIES
(
    "AUTO_INGEST" = "TRUE"
)
AS
INSERT INTO user_behavior_replica
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
); 
```

> **说明**
>
> 把上面命令示例中的 `AAA` 和 `BBB` 替换成真实有效的 Access Key 和 Secret Key 作为访问凭证。由于这里使用的数据对象对所有合法的 AWS 用户开放，因此您填入任何真实有效的 Access Key 和 Secret Key 都可以。

导入语句包含四个部分：

- `pipe_name`：Pipe 的名称。该名称在 Pipe 所在的数据库内必须唯一。
- `INSERT_SQL`：INSERT INTO SELECT FROM FILES 语句，用于从指定的源数据文件导入数据到目标表。
- `PROPERTIES`：用于控制 Pipe 执行的一些参数，例如 `AUTO_INGEST`、`POLL_INTERVAL`、`BATCH_SIZE` 和 `BATCH_FILES`，格式为 `"key" = "value"`。

有关详细的语法和参数说明，参见 [CREATE PIPE](../sql-reference/sql-statements/data-manipulation/CREATE_PIPE.md)。

#### 查看导入进度

- 通过 [SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md) 查看当前数据库中的导入作业。

  ```SQL
  SHOW PIPES;
  ```

  如果您提交了多个导入作业，您可以通过 `NAME` 过滤出想要查看哪个导入作业。例如：

  ```SQL
  SHOW PIPES WHERE NAME = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR: NULL
   CREATED_TIME: 2023-11-09 15:35:01
  1 row in set (0.01 sec)
  ```

- 通过 [`information_schema.pipes`](../reference/information_schema/pipes.md) 视图查看当前数据库中的导入作业。

  ```SQL
  SELECT * FROM information_schema.pipes;
  ```

  如果您提交了多个导入作业，您可以通过 `PIPE_NAME` 过滤出想要查看哪个导入作业。例如：

  ```SQL
  SELECT * FROM information_schema.pipes WHERE pipe_name = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR:
   CREATED_TIME: 9891-01-15 07:51:45
  1 row in set (0.01 sec)
  ```

#### 查看导入的文件信息

您可以通过 [`information_schema.pipe_files`](../reference/information_schema/pipe_files.md) 视图查看导入的文件信息。

```SQL
SELECT * FROM information_schema.pipe_files;
```

如果您提交了多个导入作业，您可以通过 `PIPE_NAME` 过滤出想要查看哪个作业下的文件信息。例如：

```SQL
SELECT * FROM information_schema.pipe_files WHERE pipe_name = 'user_behavior_replica' \G
*************************** 1. row ***************************
   DATABASE_NAME: mydatabase
         PIPE_ID: 10217
       PIPE_NAME: user_behavior_replica
       FILE_NAME: s3://starrocks-datasets/user_behavior_ten_million_rows.parquet
       FILE_VERSION: e29daa86b1120fea58ad0d047e671787-8
       FILE_SIZE: 132251298
       LAST_MODIFIED: 2023-11-06 13:25:17
      LOAD_STATE: FINISHED
     STAGED_TIME: 2023-11-09 15:35:02
 START_LOAD_TIME: 2023-11-09 15:35:03
FINISH_LOAD_TIME: 2023-11-09 15:35:42
       ERROR_MSG:
1 row in set (0.03 sec)
```

#### 管理导入作业

创建 Pipe 导入作业以后，您可以根据需要对这些作业进行修改、暂停或重新启动、删除、查询、以及尝试重新导入等操作。详情参见 [ALTER PIPE](../sql-reference/sql-statements/data-manipulation/ALTER_PIPE.md)、[SUSPEND or RESUME PIPE](../sql-reference/sql-statements/data-manipulation/SUSPEND_or_RESUME_PIPE.md)、[DROP PIPE](../sql-reference/sql-statements/data-manipulation/DROP_PIPE.md)、[SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md)、[RETRY FILE](../sql-reference/sql-statements/data-manipulation/RETRY_FILE.md)。