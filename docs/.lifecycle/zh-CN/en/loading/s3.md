---
displayed_sidebar: "Chinese"
---

# 从AWS S3加载数据

从`../assets/commonMarkdown/loadMethodIntro.md`导入LoadMethodIntro

从`../assets/commonMarkdown/insertPrivNote.md`导入InsertPrivNote

StarRocks提供以下选项用于从AWS S3加载数据：

<LoadMethodIntro />

## 开始之前

### 准备好源数据

确保您要加载到StarRocks中的源数据正确存储在S3存储桶中。您还可以考虑数据和数据库的位置，因为当您的存储桶和StarRocks集群位于同一区域时，数据传输成本要低得多。

在本主题中，我们为您提供了一个在S3存储桶中的示例数据集，位置为`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`。只要您是AWS经过身份验证的用户，就可以使用任何有效的凭证访问该对象。

### 检查权限

<InsertPrivNote />

### 收集连接详细信息

本主题中的示例使用了基于IAM用户的认证。为了确保您有权限从AWS S3中读取数据，我们建议您阅读[准备IAM用户认证](../integrations/authenticate_to_aws_resources.md)，并按照说明创建具有正确设置的IAM权限策略的IAM用户。

简而言之，如果您要使用IAM用户认证，您需要获取以下AWS资源的信息：

- 存储数据的S3存储桶。
- 如果访问存储桶中的特定对象，则需要S3对象键（对象名称）。请注意，如果您的S3对象存储在子文件夹中，则对象键可以包括前缀。
- S3存储桶所属的AWS区域。
- 用作访问凭证的访问密钥和秘密密钥。

有关所有可用认证方法的信息，请参阅[认证到AWS资源](../integrations/authenticate_to_aws_resources.md)。

## 使用INSERT+FILES()

从v3.1版本开始提供了此方法，目前仅支持Parquet和ORC文件格式。

### INSERT+FILES()的优势

[`FILES()`](../sql-reference/sql-functions/table-functions/files.md) 可根据您指定的与路径相关的属性来从云存储中读取文件，推断文件中的数据表模式，然后将文件中的数据作为数据行返回。

使用`FILES()`，您可以：

- 使用[SELECT](../sql-reference/sql-statements/data-manipulation/SELECT.md)直接从S3查询数据。
- 使用[CREATE TABLE AS SELECT](../sql-reference/sql-statements/data-definition/CREATE_TABLE_AS_SELECT.md)（CTAS）创建和加载表。
- 使用[INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md)将数据加载到现有表中。

### 典型示例

#### 使用SELECT直接从S3查询

使用SELECT+`FILES()`直接从S3查询数据可以很好地预览数据集的内容，然后再创建表。例如：

- 在不存储数据的情况下预览数据。
- 查询最小值和最大值，并决定使用什么数据类型。
- 检查`NULL`值。

以下示例查询了示例数据集`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`：\

```SQL
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
)
LIMIT 3;
```

> **注意**
>
> 请在上述命令中用您的凭证替换`AAA`和`BBB`。只要您是AWS经过身份验证的用户，就可以使用任何有效的`aws.s3.access_key`和`aws.s3.secret_key`。

系统返回以下查询结果：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
| 543711 |  829192 |    2355072 | pv           | 2017-11-27 08:22:37 |
| 543711 | 2056618 |    3645362 | pv           | 2017-11-27 10:16:46 |
| 543711 | 1165492 |    3645362 | pv           | 2017-11-27 10:17:00 |
+--------+---------+------------+--------------+---------------------+
```

> **注意**
>
> 请注意，上述返回的列名称由Parquet文件提供。

#### 使用CTAS创建和加载表

这是前面示例的延续。前面的查询包含在CREATE TABLE AS SELECT（CTAS）中，以使用模式推断自动创建表格。这意味着StarRocks将推断出表模式，创建您想要的表，然后将数据加载到表中。在使用`FILES()`表函数与Parquet文件时，不需要创建表时指定列名称和类型，因为Parquet格式包含列名称。

> **注意**
>
> 当使用模式推断时，CREATE TABLE的语法不允许设置复本数量，因此需要在创建表之前设置。下面的示例是针对有单个复本的系统：
>
> ```SQL
> ADMIN SET FRONTEND CONFIG ('default_replication_num' = "1");
> ```

创建数据库并切换到它：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

使用CTAS创建表，并将示例数据集`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`的数据加载到该表中：

```SQL
CREATE TABLE user_behavior_inferred AS
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **注意**
>
> 请在上述命令中用您的凭证替换`AAA`和`BBB`。只要您是AWS经过身份验证的用户，就可以使用任何有效的`aws.s3.access_key`和`aws.s3.secret_key`。

创建表后，您可以使用[DESCRIBE](../sql-reference/sql-statements/Utility/DESCRIBE.md)查看其模式：

```SQL
DESCRIBE user_behavior_inferred;
```

系统返回以下查询结果：

```Plaintext
+--------------+------------------+------+-------+---------+-------+
| Field        | Type             | Null | Key   | Default | Extra |
+--------------+------------------+------+-------+---------+-------+
| UserID       | bigint           | YES  | true  | NULL    |       |
| ItemID       | bigint           | YES  | true  | NULL    |       |
| CategoryID   | bigint           | YES  | true  | NULL    |       |
| BehaviorType | varchar(1048576) | YES  | false | NULL    |       |
| Timestamp    | varchar(1048576) | YES  | false | NULL    |       |
+--------------+------------------+------+-------+---------+-------+
```

将推断的模式与手动创建的模式进行比较：

- 数据类型
- 可为空
- 关键字段

为了更好地控制目标表的模式并获得更好的查询性能，我们建议您在生产环境中手动指定表模式。

查询表以验证数据是否已加载到其中。示例：

```SQL
SELECT * from user_behavior_inferred LIMIT 3;
```

返回以下查询结果，表示数据已成功加载：

```Plaintext
+--------+--------+------------+--------------+---------------------+
| UserID | ItemID | CategoryID | BehaviorType | Timestamp           |
+--------+--------+------------+--------------+---------------------+
|     58 | 158350 |    2355072 | pv           | 2017-11-27 13:06:51 |
|     58 | 158590 |    3194735 | pv           | 2017-11-27 02:21:04 |
|     58 | 215073 |    3002561 | pv           | 2017-11-30 10:55:42 |
+--------+--------+------------+--------------+---------------------+
```

#### 使用INSERT将数据加载到现有表中

您可能希望自定义要插入的表，例如：

- 列数据类型、可为空设置或默认值
- 键类型和列
- 数据分区和存储桶

> **注意**
> 创建最有效的表结构需要了解数据的使用方式和列的内容。本主题不涵盖表的设计。有关表设计的信息，请参阅[Table types](../table_design/StarRocks_table_design.md)。

在此示例中，我们根据对表的查询方式和Parquet文件中的数据的了解来创建表。可以通过直接在S3中查询文件来获得对Parquet文件中数据的了解。

- 由于对S3中数据集的查询表明`Timestamp`列包含与`datetime`数据类型匹配的数据，因此列类型在以下DDL中指定。
- 通过查询S3中的数据，可以发现数据集中没有`NULL`值，因此DDL不将任何列设置为可为空。
- 基于对预期查询类型的了解，将排序键和分桶列设置为`UserID`列。您的用例对这些数据可能有所不同，因此您可能决定对排序键使用`ItemID`以及/或者`UserID`。

创建数据库并切换到该数据库：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手动创建表（我们建议表的架构与您想要从AWS S3加载的Parquet文件具有相同的模式）：

```SQL
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

创建表之后，可以使用INSERT INTO SELECT FROM FILES()将其加载：

```SQL
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **注意**
>
> 在上述命令中用您的凭据替换`AAA`和`BBB`。任何有效的`aws.s3.access_key`和`aws.s3.secret_key`都可以使用，因为任何AWS认证用户都可以读取对象。

加载完成后，可以查询表以验证数据是否已加载到其中。例如：

```SQL
SELECT * from user_behavior_declared LIMIT 3;
```

返回以下查询结果，表明数据已成功加载：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|     58 | 4309692 |    1165503 | pv           | 2017-11-25 14:06:52 |
|     58 |  181489 |    1165503 | pv           | 2017-11-25 14:07:22 |
|     58 | 3722956 |    1165503 | pv           | 2017-11-25 14:09:28 |
+--------+---------+------------+--------------+---------------------+
```

#### 检查加载进度

您可以从[`information_schema.loads`](../reference/information_schema/loads.md)视图中查询INSERT作业的进度。此功能从v3.1版本开始受支持。例如：

```SQL
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
```

如果您已提交多个加载作业，可以根据作业关联的`LABEL`进行过滤。例如：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'insert_e3b882f5-7eb3-11ee-ae77-00163e267b60' \G
*************************** 1. row ***************************
              JOB_ID: 10243
               LABEL: insert_e3b882f5-7eb3-11ee-ae77-00163e267b60
       DATABASE_NAME: mydatabase
               STATE: FINISHED
            PROGRESS: ETL:100%; LOAD:100%
                TYPE: INSERT
            PRIORITY: NORMAL
           SCAN_ROWS: 10000000
       FILTERED_ROWS: 0
     UNSELECTED_ROWS: 0
           SINK_ROWS: 10000000
            ETL_INFO:
           TASK_INFO: resource:N/A; timeout(s):300; max_filter_ratio:0.0
         CREATE_TIME: 2023-11-09 11:56:01
      ETL_START_TIME: 2023-11-09 11:56:01
     ETL_FINISH_TIME: 2023-11-09 11:56:01
     LOAD_START_TIME: 2023-11-09 11:56:01
    LOAD_FINISH_TIME: 2023-11-09 11:56:44
         JOB_DETAILS: {"All backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[10142]},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":311710786,"InternalTableLoadRows":10000000,"ScanBytes":581574034,"ScanRows":10000000,"TaskNumber":1,"Unfinished backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[]}}
           ERROR_MSG: NULL
        TRACKING_URL: NULL
        TRACKING_SQL: NULL
REJECTED_RECORD_PATH: NULL
```

有关`loads`视图提供的字段的信息，请参阅[Information Schema](../reference/information_schema/loads.md)。

> **注意**
>
> INSERT是一个同步命令。如果INSERT作业仍在运行，您需要打开另一个会话来检查其执行状态。

## 使用Broker Load

异步Broker Load进程负责与S3建立连接、获取数据和将数据存储到StarRocks中。

此方法支持Parquet、ORC和CSV文件格式。

### Broker Load的优势

- Broker Load支持[数据转换](../loading/Etl_in_loading.md)和[数据更改，例如UPSERT和DELETE操作](../loading/Load_to_Primary_Key_tables.md)。
- Broker Load在后台运行，客户端无需保持连接以供作业继续运行。
- Broker Load适用于长时间运行的作业，默认超时时间为4小时。
- 除了Parquet和ORC文件格式，Broker Load还支持CSV文件。

### 数据流程

![Broker Load的工作流程](../assets/broker_load_how-to-work_en.png)

1. 用户创建一个加载作业。
2. 前端（FE）创建查询计划并将计划分发给后端节点（BEs）。
3. BEs从源中提取数据并将数据加载到StarRocks中。

### 典型案例

创建一个表，启动一个加载过程，从S3中拉取样本数据集`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`，然后验证数据加载的进度和成功情况。

#### 创建数据库和表

创建数据库并切换到该数据库：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手动创建表（我们建议表的架构与您要从AWS S3加载的Parquet文件具有相同的模式）：

```SQL
CREATE TABLE user_behavior
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### 启动Broker Load

运行以下命令，启动一个Broker Load作业，将样本数据集`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`的数据加载到`user_behavior`表中：

```SQL
LOAD LABEL user_behavior
(
    DATA INFILE("s3://starrocks-datasets/user_behavior_ten_million_rows.parquet")
    INTO TABLE user_behavior
    FORMAT AS "parquet"
 )
 WITH BROKER
 (
    "aws.s3.enable_ssl" = "true",
    "aws.s3.use_instance_profile" = "false",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
 )
PROPERTIES
(
    "timeout" = "72000"
);
```

> **注意**
>
> 在上述命令中用您的凭据替换`AAA`和`BBB`。任何有效的`aws.s3.access_key`和`aws.s3.secret_key`都可以使用，因为任何AWS认证用户都可以读取对象。

该作业有四个主要部分：

- `LABEL`：在查询加载作业状态时使用的字符串。
- `LOAD`声明：源URI、源数据格式和目的表名称。
- `BROKER`：源连接的详细信息。
- `PROPERTIES`：要应用于加载作业的超时值和其他属性。

有关详细的语法和参数描述，请参见[BROKER LOAD](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)。

#### 检查加载进度

您可以从[`information_schema.loads`](../reference/information_schema/loads.md)视图中查询 Broker Load 作业的进度。此功能从v3.1版开始受支持。

```SQL
SELECT * FROM information_schema.loads;
```

有关`loads`视图提供的字段的信息，请参见[信息模式](../reference/information_schema/loads.md)。

如果您已提交了多个加载作业，可以根据作业关联的`LABEL`进行筛选。例如：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'user_behavior';
```

在下面的输出中，对于加载作业`user_behavior`，有两个条目：

- 第一条记录显示为`CANCELLED`状态。查看`ERROR_MSG`，您会发现作业因`listPath failed`而失败。
- 第二条记录显示为`FINISHED`状态，这意味着该作业已成功完成。

```Plaintext
JOB_ID|LABEL                                      |DATABASE_NAME|STATE    |PROGRESS           |TYPE  |PRIORITY|SCAN_ROWS|FILTERED_ROWS|UNSELECTED_ROWS|SINK_ROWS|ETL_INFO|TASK_INFO                                           |CREATE_TIME        |ETL_START_TIME     |ETL_FINISH_TIME    |LOAD_START_TIME    |LOAD_FINISH_TIME   |JOB_DETAILS                                                                                                                                                                                                                                                    |ERROR_MSG                             |TRACKING_URL|TRACKING_SQL|REJECTED_RECORD_PATH|
------+-------------------------------------------+-------------+---------+-------------------+------+--------+---------+-------------+---------------+---------+--------+----------------------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+------------+------------+--------------------+
 10121|user_behavior                              |mydatabase   |CANCELLED|ETL:N/A; LOAD:N/A  |BROKER|NORMAL  |        0|            0|              0|        0|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:59:30|                   |                   |                   |2023-08-10 14:59:34|{"All backends":{},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":0,"InternalTableLoadRows":0,"ScanBytes":0,"ScanRows":0,"TaskNumber":0,"Unfinished backends":{}}                                                                                        |type:ETL_RUN_FAIL; msg:listPath failed|            |            |                    |
 10106|user_behavior                              |mydatabase   |FINISHED |ETL:100%; LOAD:100%|BROKER|NORMAL  | 86953525|            0|              0| 86953525|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:50:15|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:55:10|{"All backends":{"a5fe5e1d-d7d0-4826-ba99-c7348f9a5f2f":[10004]},"FileNumber":1,"FileSize":1225637388,"InternalTableLoadBytes":2710603082,"InternalTableLoadRows":86953525,"ScanBytes":1225637388,"ScanRows":86953525,"TaskNumber":1,"Unfinished backends":{"a5|                                      |            |            |                    |
```

确认加载作业已完成后，可以检查目的表的部分子集，以查看数据是否已成功加载。例如：

```SQL
SELECT * from user_behavior LIMIT 3;
```

返回以下查询结果，表明数据已成功加载：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|    142 | 2869980 |    2939262 | pv           | 2017-11-25 03:43:22 |
|    142 | 2522236 |    1669167 | pv           | 2017-11-25 15:14:12 |
|    142 | 3031639 |    3607361 | pv           | 2017-11-25 15:19:25 |
+--------+---------+------------+--------------+---------------------+
```

## 使用 Pipe

从v3.2版开始，StarRocks提供了管道加载方法，目前仅支持Parquet和ORC文件格式。

### 管道的优势

管道非常适合连续数据加载和大规模数据加载：

- **通过微批处理进行大规模数据加载有助于减少因数据错误而导致的重试成本。**

  通过管道，StarRocks能够高效加载总数据量巨大的大量数据文件。管道会根据它们的数量或大小自动分割文件，将加载作业分解为较小的顺序任务。这种方法确保了一个文件的错误不会影响整个加载作业。管道记录每个文件的加载状态，可以轻松识别和修复包含错误的文件。通过最小化由于数据错误而需要重试的次数，这种方法有助于降低成本。

- **连续数据加载有助于减少人力成本。**

  管道可以将新的或更新的数据文件写入到指定位置，并持续将这些文件中的新数据加载到StarRocks中。在为管道作业指定了`"AUTO_INGEST" = "TRUE"`之后，管道将持续监视指定路径中存储的数据文件的更改，并自动从这些文件中加载新的或更新的数据到目的地StarRocks表中。

此外，管道执行文件唯一性检查，有助于防止重复数据加载。在加载过程中，管道根据文件名和摘要检查每个数据文件的唯一性。如果具有特定文件名和摘要的文件已经被管道作业处理过，管道作业将跳过所有后续具有相同文件名和摘要的文件。请注意，像AWS S3这样的对象存储使用`ETag`作为文件摘要。

每个数据文件的加载状态都记录在`information_schema.pipe_files`视图中。与该视图关联的管道作业被删除后，有关在该作业中加载的文件的记录也将被删除。

### 数据流

![管道数据流](../assets/pipe_data_flow.png)

### 管道与INSERT+FILES()的区别

一个管道作业根据每个数据文件的大小和行数分成一个或多个事务。用户可以在加载过程中查询中间结果。相反，一个INSERT+`FILES()`作业被处理为单个事务，用户无法在加载过程中查看数据。

### 文件加载顺序

对于每个管道作业，StarRocks维护一个文件队列，从中获取和加载数据文件作为微批处理。管道不确保数据文件以上传的顺序进行加载。因此，较新的数据可能会先于旧数据加载。

### 典型示例

#### 创建数据库和表

创建一个数据库并切换到它：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手动创建一个表（我们建议表的架构与您要从AWS S3加载的Parquet文件具有相同的架构）：

```SQL
CREATE TABLE user_behavior_replica
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### 启动管道作业

运行以下命令启动一个管道作业，将样例数据集`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`中的数据加载到`user_behavior_replica`表：

```SQL
CREATE PIPE user_behavior_replica
PROPERTIES
(
    "AUTO_INGEST" = "TRUE"
)
AS
INSERT INTO user_behavior_replica
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
); 
```

> **注意**
>
> 在上述命令中，将您的凭据替换为 `AAA` 和 `BBB`。可以使用任何有效的 `aws.s3.access_key` 和 `aws.s3.secret_key`，因为该对象可被任何 AWS 认证用户读取。

此作业有四个主要部分：

- `pipe_name`：管道的名称。管道名称必须在其所属的数据库中是唯一的。
- `INSERT_SQL`：用于将数据从指定源数据文件加载到目标表的 INSERT INTO SELECT FROM FILES 语句。
- `PROPERTIES`：一组可选参数，用于指定如何执行管道。这些参数包括 `AUTO_INGEST`、`POLL_INTERVAL`、`BATCH_SIZE` 和 `BATCH_FILES`。以 `"key" = "value"` 的格式指定这些属性。

有关详细的语法和参数描述，请参见 [CREATE PIPE](../sql-reference/sql-statements/data-manipulation/CREATE_PIPE.md)。

#### 检查加载进度

- 通过使用 [SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md) 查询管道作业的进度。

  ```SQL
  SHOW PIPES;
  ```

  如果您已提交多个加载作业，您可以根据作业关联的 `NAME` 进行筛选。示例：

  ```SQL
  SHOW PIPES WHERE NAME = 'user_behavior_replica' \G
  *************************** 1. 行 ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR: NULL
   CREATED_TIME: 2023-11-09 15:35:01
  1 行 (0.01 秒)
  ```

- 从 [`information_schema.pipes`](../reference/information_schema/pipes.md) 视图中查询管道作业的进度。

  ```SQL
  SELECT * FROM information_schema.pipes;
  ```

  如果您已提交多个加载作业，您可以根据作业关联的 `PIPE_NAME` 进行筛选。示例：

  ```SQL
  SELECT * FROM information_schema.pipes WHERE pipe_name = 'user_behavior_replica' \G
  *************************** 1. 行 ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR:
   CREATED_TIME: 9891-01-15 07:51:45
  1 行 (0.01 秒)
  ```

#### 检查文件状态

您可以从 [`information_schema.pipe_files`](../reference/information_schema/pipe_files.md) 视图中查询从文件加载的文件的加载状态。

```SQL
SELECT * FROM information_schema.pipe_files;
```

如果您已提交多个加载作业，您可以根据作业关联的 `PIPE_NAME` 进行筛选。示例：

```SQL
SELECT * FROM information_schema.pipe_files WHERE pipe_name = 'user_behavior_replica' \G
*************************** 1. 行 ***************************
   DATABASE_NAME: mydatabase
         PIPE_ID: 10217
       PIPE_NAME: user_behavior_replica
       FILE_NAME: s3://starrocks-datasets/user_behavior_ten_million_rows.parquet
    FILE_VERSION: e29daa86b1120fea58ad0d047e671787-8
       FILE_SIZE: 132251298
   LAST_MODIFIED: 2023-11-06 13:25:17
      LOAD_STATE: FINISHED
     STAGED_TIME: 2023-11-09 15:35:02
 START_LOAD_TIME: 2023-11-09 15:35:03
FINISH_LOAD_TIME: 2023-11-09 15:35:42
       ERROR_MSG:
1 行 (0.03 秒)
```

#### 管理管道作业

您可以更改、暂停或恢复、删除或查询您创建的管道，并重试加载特定数据文件。有关更多信息，请参见 [ALTER PIPE](../sql-reference/sql-statements/data-manipulation/ALTER_PIPE.md)、[SUSPEND or RESUME PIPE](../sql-reference/sql-statements/data-manipulation/SUSPEND_or_RESUME_PIPE.md)、[DROP PIPE](../sql-reference/sql-statements/data-manipulation/DROP_PIPE.md)、[SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md) 和 [RETRY FILE](../sql-reference/sql-statements/data-manipulation/RETRY_FILE.md)。