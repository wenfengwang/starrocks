---
displayed_sidebar: "Japanese"
---

# AWS S3からデータをロードする

StarRocksでは、以下のオプションを使用してAWS S3からデータをロードすることができます。

- [INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md)+[`FILES()`](../sql-reference/sql-functions/table-functions/files.md)を使用した同期ロード
- [Broker Load](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)を使用した非同期ロード
- [Pipe](../sql-reference/sql-statements/data-manipulation/CREATE_PIPE.md)を使用した連続非同期ロード

これらのオプションはそれぞれ独自の利点を持っており、以下のセクションで詳細に説明します。

ほとんどの場合、INSERT+`FILES()`メソッドの使用をお勧めします。このメソッドは非常に使いやすく、ParquetおよびORCファイル形式のみをサポートしています。

ただし、INSERT+`FILES()`メソッドは現在、ParquetおよびORCファイル形式のみをサポートしています。そのため、CSVなどの他のファイル形式のデータをロードする必要がある場合や、データのロード中に[DELETEなどのデータ変更を行う](../loading/Load_to_Primary_Key_tables.md)場合は、Broker Loadを使用することができます。

また、合計で100 GB以上、または1 TB以上のデータボリュームを持つ大量のデータファイルをロードする必要がある場合は、Pipeメソッドを使用することをお勧めします。Pipeはファイルの数やサイズに基づいてファイルを分割し、ロードジョブをより小さな連続的なタスクに分割します。これにより、1つのファイルのエラーがロードジョブ全体に影響を与えることがなくなり、データエラーによる再試行の必要性を最小限に抑えることができます。

## 開始する前に

### ソースデータを準備する

StarRocksにロードするソースデータがS3バケットに適切に保存されていることを確認してください。また、データとデータベースの場所を考慮することも重要です。バケットとStarRocksクラスタが同じリージョンにある場合、データ転送コストが大幅に低くなります。

このトピックでは、S3バケットの`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`にサンプルデータセットを提供します。このデータセットは、AWSの認証済みユーザーであれば誰でも読み取り可能なオブジェクトとしてアクセスできます。

### 権限を確認する

StarRocksテーブルにデータをロードするには、そのStarRocksテーブルにINSERT権限を持つユーザーとしてのみデータをロードすることができます。INSERT権限がない場合は、StarRocksクラスタに接続するために使用するユーザーにINSERT権限を付与するための[GRANT](../sql-reference/sql-statements/account-management/GRANT.md)の手順に従ってください。

### 接続の詳細を収集する

このトピックの例では、IAMユーザーベースの認証を使用しています。AWS S3からデータを読み取るための権限を持っていることを確認するために、[IAMユーザーベースの認証の準備](../integrations/authenticate_to_aws_resources.md)を読み、適切に設定された[IAMポリシー](../reference/aws_iam_policies.md)を持つIAMユーザーを作成するための手順に従ってください。

要するに、IAMユーザーベースの認証を使用する場合、次のAWSリソースに関する情報を収集する必要があります。

- データを保存するS3バケット。
- バケット内の特定のオブジェクトにアクセスする場合は、S3オブジェクトキー（オブジェクト名）。S3オブジェクトがサブフォルダに保存されている場合、オブジェクトキーにはプレフィックスを含めることができます。
- S3バケットが所属するAWSリージョン。
- アクセス認証に使用されるアクセスキーとシークレットキー。

利用可能なすべての認証方法の詳細については、[AWSリソースへの認証](../integrations/authenticate_to_aws_resources.md)を参照してください。

## INSERT+FILES()を使用する

このメソッドはv3.1以降で使用可能で、現在はParquetおよびORCファイル形式のみをサポートしています。

### INSERT+FILES()の利点

[`FILES()`](../sql-reference/sql-functions/table-functions/files.md)は、指定したパス関連のプロパティに基づいてクラウドストレージに保存されているファイルを読み取り、ファイル内のデータのテーブルスキーマを推測し、そのデータをデータ行として返すことができます。

`FILES()`を使用すると、次のことができます。

- [SELECT](../sql-reference/sql-statements/data-manipulation/SELECT.md)を使用してS3から直接データをクエリする。
- [CREATE TABLE AS SELECT](../sql-reference/sql-statements/data-definition/CREATE_TABLE_AS_SELECT.md)（CTAS）を使用してテーブルを作成し、データをロードする。
- [INSERT](../sql-reference/sql-statements/data-manipulation/INSERT.md)を使用して既存のテーブルにデータをロードする。

### 典型的な例

#### SELECTを使用してS3から直接クエリする

SELECT+`FILES()`を使用してS3から直接クエリを実行することで、テーブルの内容をプレビューすることができます。例えば：

- データを保存せずにデータセットのプレビューを取得する。
- 最小値と最大値をクエリし、使用するデータ型を決定する。
- `NULL`値をチェックする。

次の例では、サンプルデータセット`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`をクエリしています：

```SQL
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
)
LIMIT 3;
```

> **注意**
>
> 上記のコマンドの`AAA`と`BBB`を自分の認証情報に置き換えてください。有効な`aws.s3.access_key`と`aws.s3.secret_key`はどれでも使用できます。オブジェクトはAWSの認証済みユーザーであれば誰でも読み取り可能です。

システムは以下のクエリ結果を返します：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
| 543711 |  829192 |    2355072 | pv           | 2017-11-27 08:22:37 |
| 543711 | 2056618 |    3645362 | pv           | 2017-11-27 10:16:46 |
| 543711 | 1165492 |    3645362 | pv           | 2017-11-27 10:17:00 |
+--------+---------+------------+--------------+---------------------+
```

> **注意**
>
> 上記の結果で返される列名は、Parquetファイルによって提供されることに注意してください。

#### CTASを使用してテーブルを作成し、データをロードする

これは前の例の続きです。前のクエリをCREATE TABLE AS SELECT（CTAS）でラップして、スキーマ推測を使用してテーブルを自動的に作成し、データをテーブルにロードします。Parquet形式では、列名は必要ありません。

> **注意**
>
> スキーマ推測を使用する場合のCREATE TABLEの構文では、レプリカの数を設定することはできないため、テーブルを作成する前に設定してください。以下の例は、シングルレプリカのシステムの場合です：
>
> ```SQL
> ADMIN SET FRONTEND CONFIG ('default_replication_num' = "1");
> ```

データベースを作成し、それに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

CTASを使用してテーブルを作成し、サンプルデータセット`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`のデータをテーブルにロードします：

```SQL
CREATE TABLE user_behavior_inferred AS
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **注意**
>
> 上記のコマンドの`AAA`と`BBB`を自分の認証情報に置き換えてください。有効な`aws.s3.access_key`と`aws.s3.secret_key`はどれでも使用できます。オブジェクトはAWSの認証済みユーザーであれば誰でも読み取り可能です。

テーブルを作成した後、[DESCRIBE](../sql-reference/sql-statements/Utility/DESCRIBE.md)を使用してスキーマを表示できます：

```SQL
DESCRIBE user_behavior_inferred;
```

システムは以下のクエリ結果を返します：

```Plaintext
+--------------+------------------+------+-------+---------+-------+
| Field        | Type             | Null | Key   | Default | Extra |
+--------------+------------------+------+-------+---------+-------+
| UserID       | bigint           | YES  | true  | NULL    |       |
| ItemID       | bigint           | YES  | true  | NULL    |       |
| CategoryID   | bigint           | YES  | true  | NULL    |       |
| BehaviorType | varchar(1048576) | YES  | false | NULL    |       |
| Timestamp    | varchar(1048576) | YES  | false | NULL    |       |
+--------------+------------------+------+-------+---------+-------+
```

スキーマ推測と手動で作成したスキーマを比較してください：

- データ型
- NULL可能
- キーフィールド

生産環境では、宛先テーブルのスキーマをよりよく制御し、クエリのパフォーマンスを向上させるために、スキーマを手動で指定することをお勧めします。

データが正常にロードされたことを確認するために、テーブルをクエリしてデータを確認できます。例：

```SQL
SELECT * from user_behavior_inferred LIMIT 3;
```

以下のクエリ結果が返され、データが正常にロードされていることが示されます：

```Plaintext
+--------+--------+------------+--------------+---------------------+
| UserID | ItemID | CategoryID | BehaviorType | Timestamp           |
+--------+--------+------------+--------------+---------------------+
|     58 | 158350 |    2355072 | pv           | 2017-11-27 13:06:51 |
|     58 | 158590 |    3194735 | pv           | 2017-11-27 02:21:04 |
|     58 | 215073 |    3002561 | pv           | 2017-11-30 10:55:42 |
+--------+--------+------------+--------------+---------------------+
```

#### INSERTを使用して既存のテーブルにデータをロードする

挿入先のテーブルをカスタマイズする場合があります。例えば、次のような場合です：

- 列のデータ型、NULL設定、デフォルト値をカスタマイズする。
- キータイプと列をカスタマイズする。
- データのパーティショニングとバケット化をカスタマイズする。

> **注意**
>
> 最も効率的なテーブル構造を作成するには、データの使用方法と列の内容を知る必要があります。このトピックではテーブル設計については説明しません。テーブル設計については、[テーブルの種類](../table_design/StarRocks_table_design.md)を参照してください。

この例では、クエリのタイプと列の内容に基づいてテーブルを作成しています。S3でデータをクエリすることで、データの内容を知ることができます。

- S3のデータセットのクエリ結果から、`Timestamp`列には`datetime`データ型に一致するデータが含まれていることがわかりますので、以下のDDLで列の型を指定しています。
- S3でデータをクエリすることで、データセットに`NULL`値がないことがわかるため、DDLでは列をNULL可能としていません。
- 予想されるクエリタイプに基づいて、ソートキーとバケット化列を`UserID`に設定しています。データによっては、このデータに対して異なる使用方法をするため、ソートキーとして`UserID`の代わりに`ItemID`を使用するか、または`UserID`に加えて`ItemID`を使用するかを決定するかもしれません。

データベースを作成し、それに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手動でテーブルを作成します（Parquetファイルからロードする場合、テーブルと同じスキーマを持つことをお勧めします）：

```SQL
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

テーブルを作成した後、INSERT INTO SELECT FROM FILES()でテーブルにデータをロードできます：

```SQL
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **注意**
>
> 上記のコマンドの`AAA`と`BBB`を自分の認証情報に置き換えてください。有効な`aws.s3.access_key`と`aws.s3.secret_key`はどれでも使用できます。オブジェクトはAWSの認証済みユーザーであれば誰でも読み取り可能です。

ロードが完了したら、テーブルをクエリしてデータが正常にロードされたかどうかを確認できます。例：

```SQL
SELECT * from user_behavior_declared LIMIT 3;
```

以下のクエリ結果が返され、データが正常にロードされていることが示されます：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|     58 | 4309692 |    1165503 | pv           | 2017-11-25 14:06:52 |
|     58 |  181489 |    1165503 | pv           | 2017-11-25 14:07:22 |
|     58 | 3722956 |    1165503 | pv           | 2017-11-25 14:09:28 |
+--------+---------+------------+--------------+---------------------+
```

#### ロードの進捗状況を確認する

[`information_schema.loads`](../reference/information_schema/loads.md)ビューからINSERTジョブの進捗状況をクエリできます。この機能はv3.1以降でサポートされています。例：

```SQL
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
```

複数のロードジョブを送信した場合、ジョブに関連付けられた`LABEL`でフィルタリングすることができます。例：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'insert_e3b882f5-7eb3-11ee-ae77-00163e267b60' \G
*************************** 1. row ***************************
              JOB_ID: 10243
               LABEL: insert_e3b882f5-7eb3-11ee-ae77-00163e267b60
       DATABASE_NAME: mydatabase
               STATE: FINISHED
            PROGRESS: ETL:100%; LOAD:100%
                TYPE: INSERT
            PRIORITY: NORMAL
           SCAN_ROWS: 10000000
       FILTERED_ROWS: 0
     UNSELECTED_ROWS: 0
           SINK_ROWS: 10000000
            ETL_INFO:
           TASK_INFO: resource:N/A; timeout(s):300; max_filter_ratio:0.0
         CREATE_TIME: 2023-11-09 11:56:01
      ETL_START_TIME: 2023-11-09 11:56:01
     ETL_FINISH_TIME: 2023-11-09 11:56:01
     LOAD_START_TIME: 2023-11-09 11:56:01
    LOAD_FINISH_TIME: 2023-11-09 11:56:44
         JOB_DETAILS: {"All backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[10142]},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":311710786,"InternalTableLoadRows":10000000,"ScanBytes":581574034,"ScanRows":10000000,"TaskNumber":1,"Unfinished backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[]}}
           ERROR_MSG: NULL
        TRACKING_URL: NULL
        TRACKING_SQL: NULL
REJECTED_RECORD_PATH: NULL
```

`loads`ビューで提供されるフィールドの詳細については、[Information Schema](../reference/information_schema/loads.md)を参照してください。

## Broker Loadを使用する

非同期のBroker Loadプロセスは、S3への接続、データの取得、およびデータのStarRocksへの保存を処理します。

このメソッドはParquet、ORC、およびCSVファイル形式をサポートしています。

### Broker Loadの利点

- Broker Loadは、ロード中に[データ変換](../loading/Etl_in_loading.md)や[UPSERTおよびDELETE操作などのデータ変更](../loading/Load_to_Primary_Key_tables.md)をサポートしています。
- Broker Loadはバックグラウンドで実行され、ジョブが継続するためにクライアントが接続し続ける必要はありません。
- Broker Loadは、デフォルトのタイムアウトが4時間に設定されている長時間実行ジョブに適しています。
- ParquetおよびORCファイル形式に加えて、Broker LoadはCSVファイルもサポートしています。

### データフロー

![Broker Loadのワークフロー](../assets/broker_load_how-to-work_en.png)

1. ユーザーがロードジョブを作成します。
2. フロントエンド（FE）がクエリプランを作成し、プランをバックエンドノード（BE）に配布します。
3. BEがソースからデータを取得し、データをStarRocksにロードします。

### 典型的な例

#### データベースとテーブルを作成する

データベースを作成し、それに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手動でテーブルを作成します（Parquetファイルからロードする場合、テーブルと同じスキーマを持つことをお勧めします）：

```SQL
CREATE TABLE user_behavior
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### Broker Loadを開始する

次のコマンドを実行して、サンプルデータセット`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`から`user_behavior`テーブルにデータをロードするBroker Loadジョブを開始します：

```SQL
LOAD LABEL user_behavior
(
    DATA INFILE("s3://starrocks-datasets/user_behavior_ten_million_rows.parquet")
    INTO TABLE user_behavior
    FORMAT AS "parquet"
 )
 WITH BROKER
 (
    "aws.s3.enable_ssl" = "true",
    "aws.s3.use_instance_profile" = "false",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
 )
PROPERTIES
(
    "timeout" = "72000"
);
```

> **注意**
>
> 上記のコマンドの`AAA`と`BBB`を自分の認証情報に置き換えてください。有効な`aws.s3.access_key`と`aws.s3.secret_key`はどれでも使用できます。オブジェクトはAWSの認証済みユーザーであれば誰でも読み取り可能です。

このジョブには4つのメインセクションがあります：

- `LABEL`：ジョブの状態をクエリする際に使用する文字列です。
- `LOAD`宣言：ソースURI、ソースデータ形式、および宛先テーブル名です。
- `BROKER`：ソースの接続詳細です。
- `PROPERTIES`：タイムアウト値とロードジョブに適用するその他のプロパティです。

詳細な構文とパラメータの説明については、[BROKER LOAD](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)を参照してください。

#### ロードの進捗状況を確認する

[`information_schema.loads`](../reference/information_schema/loads.md)ビューからBroker Loadジョブの進捗状況をクエリできます。この機能はv3.1以降でサポートされています。

```SQL
SELECT * FROM information_schema.loads;
```

`loads`ビューで提供されるフィールドの詳細については、[Information Schema](../reference/information_schema/loads.md)を参照してください。

複数のロードジョブを送信した場合、ジョブに関連付けられた`LABEL`でフィルタリングすることができます。例：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'user_behavior';
```

以下の出力には、ロードジョブ`user_behavior`に関する2つのエントリがあります：

- 最初のレコードは`CANCELLED`の状態を示しています。`ERROR_MSG`にスクロールすると、`listPath failed`のエラーによりジョブが失敗したことがわかります。
- 2番目のレコードは`FINISHED`の状態を示しており、ジョブが成功したことを意味します。

```Plaintext
JOB_ID|LABEL                                      |DATABASE_NAME|STATE    |PROGRESS           |TYPE  |PRIORITY|SCAN_ROWS|FILTERED_ROWS|UNSELECTED_ROWS|SINK_ROWS|ETL_INFO|TASK_INFO                                           |CREATE_TIME        |ETL_START_TIME     |ETL_FINISH_TIME    |LOAD_START_TIME    |LOAD_FINISH_TIME   |JOB_DETAILS                                                                                                                                                                                                                                                    |ERROR_MSG                             |TRACKING_URL|TRACKING_SQL|REJECTED_RECORD_PATH|
------+-------------------------------------------+-------------+---------+-------------------+------+--------+---------+-------------+---------------+---------+--------+----------------------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+------------+------------+--------------------+
 10121|user_behavior                              |mydatabase   |CANCELLED|ETL:N/A; LOAD:N/A  |BROKER|NORMAL  |        0|            0|              0|        0|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:59:30|                   |                   |                   |2023-08-10 14:59:34|{"All backends":{},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":0,"InternalTableLoadRows":0,"ScanBytes":0,"ScanRows":0,"TaskNumber":0,"Unfinished backends":{}}                                                                                        |type:ETL_RUN_FAIL; msg:listPath failed|            |            |                    |
 10106|user_behavior                              |mydatabase   |FINISHED |ETL:100%; LOAD:100%|BROKER|NORMAL  | 86953525|            0|              0| 86953525|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:50:15|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:55:10|{"All backends":{"a5fe5e1d-d7d0-4826-ba99-c7348f9a5f2f":[10004]},"FileNumber":1,"FileSize":1225637388,"InternalTableLoadBytes":2710603082,"InternalTableLoadRows":86953525,"ScanBytes":1225637388,"ScanRows":86953525,"TaskNumber":1,"Unfinished backends":{"a5|                                      |            |            |                    |
```

ロードジョブが完了したことを確認した後、宛先テーブルの一部を確認してデータが正常にロードされたかどうかを確認できます。例：

```SQL
SELECT * from user_behavior LIMIT 3;
```

以下のクエリ結果が返され、データが正常にロードされていることが示されます：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|    142 | 2869980 |    2939262 | pv           | 2017-11-25 03:43:22 |
|    142 | 2522236 |    1669167 | pv           | 2017-11-25 15:14:12 |
|    142 | 3031639 |    3607361 | pv           | 2017-11-25 15:19:25 |
+--------+---------+------------+--------------+---------------------+
```

## Pipeを使用する

v3.2以降、StarRocksはPipeを使用してAWS S3からデータを連続的にマイクロバッチでロードすることができます。これにより、ロードしたデータにすばやくアクセスできるようになります。定期的にロードコマンドを手動で実行する必要がなくなるため、連続的なデータロードや大規模なデータロードに最適です。

このメソッドは現在、ParquetおよびORCファイル形式のみをサポートしています。

### Pipeの利点

- **マイクロバッチでの大規模データロードにより、データエラーによるリトライのコストを削減できます。**

  Pipeのおかげで、合計で大量のデータファイルを効率的にロードすることができます。Pipeはファイルの数やサイズに基づいてファイルを自動的に分割し、ロードジョブをより小さな連続的なタスクに分割します。これにより、1つのファイルのエラーがロードジョブ全体に影響を与えることがなくなります。Pipeは各ファイルのロードステータスを記録し、エラーを含むファイルを簡単に特定して修正することができます。データエラーによるリトライの必要性を最小限に抑えることで、コストを削減することができます。

- **連続的なデータロードにより、人的手間を削減できます。**

  Pipeを使用すると、新しいデータファイルを特定の場所に書き込み、これらのファイルから新しいデータをStarRocksに連続的にロードすることができます。Pipeジョブが作成されると、特定のディレクトリに保存されているデータファイルの変更を常に監視し、データファイルから新しいデータまたは更新されたデータを自動的にStarRocksの宛先テーブルにロードします。

- **ファイルの一意性チェックにより、重複したデータのロードを防ぐことができます。**

  ロードプロセス中、Pipeはファイル名とダイジェストに基づいて各データファイルの一意性をチェックします。特定のファイル名とダイジェストを持つファイルがPipeジョブによってすでに処理されている場合、Pipeジョブは同じファイル名とダイジェストを持つすべての後続のファイルをスキップします。オブジェクトストレージ（AWS S3など）では、ファイルのダイジェストとして`ETag`が使用されます。

各データファイルのロードステータスは、`information_schema.pipe_files`ビューに記録および保存されます。ビューに関連付けられたPipeジョブが削除されると、そのジョブでロードされたファイルに関するレコードも削除されます。

### データフロー

![Pipeデータフロー](../assets/pipe_data_flow.png)

### PipeとINSERT+FILES()の違い

Pipeジョブは、各データファイルのサイズと行数に基づいて1つ以上のトランザクションに分割されます。ユーザーはロードプロセス中に中間結果をクエリすることができます。一方、INSERT+`FILES()`ジョブは単一のトランザクションとして処理され、ユーザーはロードプロセス中にデータを表示することはできません。

### ファイルのロード順序

各Pipeジョブには、ファイルキューがあり、マイクロバッチとしてデータファイルを取得してロードします。Pipeは、データファイルがアップロードされた順序でロードされることを保証しません。したがって、新しいデータは古いデータよりも先にロードされる場合があります。

### 典型的な例

#### データベースとテーブルを作成する

データベースを作成し、それに切り替えます：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手動でテーブルを作成します（Parquetファイルからロードする場合、テーブルと同じスキーマを持つことをお勧めします）：

```SQL
CREATE TABLE user_behavior_replica
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### Pipeジョブを開始する

次のコマンドを実行して、サンプルデータセット`s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`から`user_behavior_replica`テーブルにデータをロードするPipeジョブを開始します：

```SQL
CREATE PIPE user_behavior_replica
PROPERTIES
(
    "AUTO_INGEST" = "TRUE"
)
AS
INSERT INTO user_behavior_replica
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
); 
```

> **注意**
>
> 上記のコマンドの`AAA`と`BBB`を自分の認証情報に置き換えてください。有効な`aws.s3.access_key`と`aws.s3.secret_key`はどれでも使用できます。オブジェクトはAWSの認証済みユーザーであれば誰でも読み取り可能です。

このジョブには4つのメインセクションがあります：

- `pipe_name`：パイプの名前。パイプ名は、パイプが所属するデータベース内で一意である必要があります。
- `INSERT_SQL`: 指定されたソースデータファイルから宛先テーブルにデータをロードするために使用されるINSERT INTO SELECT FROM FILESステートメントです。
- `PROPERTIES`: パイプを実行する方法を指定するオプションのパラメータセットです。これには、`AUTO_INGEST`、`POLL_INTERVAL`、`BATCH_SIZE`、`BATCH_FILES`が含まれます。これらのプロパティは、`"key" = "value"`の形式で指定します。

詳細な構文とパラメータの説明については、[CREATE PIPE](../sql-reference/sql-statements/data-manipulation/CREATE_PIPE.md)を参照してください。

#### ロードの進捗状況を確認する

- [SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md)を使用して、パイプジョブの進捗状況をクエリします。

  ```SQL
  SHOW PIPES;
  ```

  複数のロードジョブを送信した場合、ジョブに関連付けられた`NAME`でフィルタリングすることができます。例：

  ```SQL
  SHOW PIPES WHERE NAME = "user_behavior_replica" \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR: NULL
   CREATED_TIME: 2023-11-09 15:35:01
  1 row in set (0.01 sec)
  ```

- [`information_schema.pipes`](../reference/information_schema/pipes.md)ビューからパイプジョブの進捗状況をクエリします。

  ```SQL
  SELECT * FROM information_schema.pipes;
  ```

  複数のロードジョブを送信した場合、ジョブに関連付けられた`PIPE_NAME`でフィルタリングすることができます。例：

  ```SQL
  SELECT * FROM information_schema.pipes WHERE pipe_name = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR:
   CREATED_TIME: 9891-01-15 07:51:45
  1 row in set (0.01 sec)
  ```

#### ファイルのステータスを確認する

[`information_schema.pipe_files`](../reference/information_schema/pipe_files.md)ビューから、ロードされたファイルのロードステータスをクエリできます。

```SQL
SELECT * FROM information_schema.pipe_files;
```

複数のロードジョブを送信した場合、ジョブに関連付けられた`PIPE_NAME`でフィルタリングすることができます。例：

```SQL
SELECT * FROM information_schema.pipe_files WHERE pipe_name = 'user_behavior_replica' \G
*************************** 1. row ***************************
   DATABASE_NAME: mydatabase
         PIPE_ID: 10217
       PIPE_NAME: user_behavior_replica
       FILE_NAME: s3://starrocks-datasets/user_behavior_ten_million_rows.parquet
    FILE_VERSION: e29daa86b1120fea58ad0d047e671787-8
       FILE_SIZE: 132251298
   LAST_MODIFIED: 2023-11-06 13:25:17
      LOAD_STATE: FINISHED
     STAGED_TIME: 2023-11-09 15:35:02
 START_LOAD_TIME: 2023-11-09 15:35:03
FINISH_LOAD_TIME: 2023-11-09 15:35:42
       ERROR_MSG:
1 row in set (0.03 sec)
```

#### パイプジョブの管理

作成したパイプを変更、中断または再開、削除、またはクエリすることができます。詳細については、[ALTER PIPE](../sql-reference/sql-statements/data-manipulation/ALTER_PIPE.md)、[SUSPEND or RESUME PIPE](../sql-reference/sql-statements/data-manipulation/SUSPEND_or_RESUME_PIPE.md)、[DROP PIPE](../sql-reference/sql-statements/data-manipulation/DROP_PIPE.md)、[SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md)、および[RETRY FILE](../sql-reference/sql-statements/data-manipulation/RETRY_FILE.md)を参照してください。
