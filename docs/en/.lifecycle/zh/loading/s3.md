---
displayed_sidebar: English
---

```SQL
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

创建表后，可以使用 INSERT INTO SELECT FROM FILES() 将其加载：

```SQL
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **笔记**
> 在上述命令中将您的凭据替换为 `AAA` 和 `BBB`。可以使用任何有效的 `aws.s3.access_key` 和 `aws.s3.secret_key`，因为任何经过 AWS 身份验证的用户都可以读取该对象。

加载完成后，可以查询表以验证数据是否已加载到其中。例子：

```SQL
SELECT * from user_behavior_declared LIMIT 3;
```

返回如下查询结果，说明数据已成功加载：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|     58 | 4309692 |    1165503 | pv           | 2017-11-25 14:06:52 |
|     58 |  181489 |    1165503 | pv           | 2017-11-25 14:07:22 |
|     58 | 3722956 |    1165503 | pv           | 2017-11-25 14:09:28 |
+--------+---------+------------+--------------+---------------------+
```

#### 检查加载进度

您可以从 [`information_schema.loads`](../reference/information_schema/loads.md) 视图查询 INSERT 作业的进度。此功能从 v3.1 开始支持。示例：

```SQL
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
```

如果您提交了多个加载作业，可以根据与作业关联的 `LABEL` 进行筛选。示例：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'insert_e3b882f5-7eb3-11ee-ae77-00163e267b60' \G
*************************** 1. row ***************************
              JOB_ID: 10243
               LABEL: insert_e3b882f5-7eb3-11ee-ae77-00163e267b60
       DATABASE_NAME: mydatabase
               STATE: FINISHED
            PROGRESS: ETL:100%; LOAD:100%
                TYPE: INSERT
            PRIORITY: NORMAL
           SCAN_ROWS: 10000000
       FILTERED_ROWS: 0
     UNSELECTED_ROWS: 0
           SINK_ROWS: 10000000
            ETL_INFO:
           TASK_INFO: resource:N/A; timeout(s):300; max_filter_ratio:0.0
         CREATE_TIME: 2023-11-09 11:56:01
      ETL_START_TIME: 2023-11-09 11:56:01
     ETL_FINISH_TIME: 2023-11-09 11:56:01
     LOAD_START_TIME: 2023-11-09 11:56:01
    LOAD_FINISH_TIME: 2023-11-09 11:56:44
         JOB_DETAILS: {"All backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[10142]},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":311710786,"InternalTableLoadRows":10000000,"ScanBytes":581574034,"ScanRows":10000000,"TaskNumber":1,"Unfinished backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[]}}
           ERROR_MSG: NULL
        TRACKING_URL: NULL
        TRACKING_SQL: NULL
REJECTED_RECORD_PATH: NULL
```

有关 `loads` 视图提供的字段的信息，请参阅 [Information Schema](../reference/information_schema/loads.md)。

> **笔记**
> INSERT 是一个同步命令。如果 INSERT 作业仍在运行，您需要打开另一个会话来检查其执行状态。

## 使用 Broker Load

异步 Broker Load 进程负责建立与 S3 的连接、拉取数据并将数据存储到 StarRocks 中。

此方法支持 Parquet、ORC 和 CSV 文件格式。

### Broker Load 的优点

- Broker Load 支持[数据转换](../loading/Etl_in_loading.md)和[数据更改（如 UPSERT 和 DELETE 操作）](../loading/Load_to_Primary_Key_tables.md)。
- Broker Load 在后台运行，客户端无需保持连接以继续作业。
- Broker Load 适用于长时间运行的作业，默认超时时间为 4 小时。
- 除了 Parquet 和 ORC 文件格式，Broker Load 还支持 CSV 文件。

### 数据流

![Broker Load 工作流程](../assets/broker_load_how-to-work_en.png)

1. 用户创建一个加载作业。
2. 前端（FE）创建一个查询计划并将计划分发给后端节点（BE）。
3. BE 从源中拉取数据并将数据加载到 StarRocks 中。

### 典型例子

#### 创建数据库和表

创建数据库并切换到它：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手动创建一个表（我们建议表的架构与要从 AWS S3 加载的 Parquet 文件相同）：

```SQL
CREATE TABLE user_behavior
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### 启动 Broker Load

运行以下命令以启动一个 Broker Load 作业，该作业从 S3 拉取示例数据集 `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` 并将其加载到 `user_behavior` 表中：

```SQL
LOAD LABEL user_behavior
(
    DATA INFILE("s3://starrocks-datasets/user_behavior_ten_million_rows.parquet")
    INTO TABLE user_behavior
    FORMAT AS "parquet"
 )
 WITH BROKER
 (
    "aws.s3.enable_ssl" = "true",
    "aws.s3.use_instance_profile" = "false",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
 )
PROPERTIES
(
    "timeout" = "72000"
);
```

> **笔记**
> 在上述命令中将您的凭据替换为 `AAA` 和 `BBB`。可以使用任何有效的 `aws.s3.access_key` 和 `aws.s3.secret_key`，因为任何经过 AWS 身份验证的用户都可以读取该对象。

此作业有四个主要部分：

- `LABEL`：用于查询加载作业状态的字符串。
- `LOAD` 声明：指定源数据文件、源数据格式和目标表名的 INSERT INTO SELECT FROM FILES 语句。
- `BROKER`：指定源的连接详细信息。
- `PROPERTIES`：指定加载作业的超时值和其他属性的一组可选参数。

有关详细的语法和参数描述，请参阅 [BROKER LOAD](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)。

#### 检查加载进度

您可以从 [`information_schema.loads`](../reference/information_schema/loads.md) 视图查询 Broker Load 作业的进度。此功能从 v3.1 开始支持。

```SQL
SELECT * FROM information_schema.loads;
```

有关 `loads` 视图提供的字段的信息，请参阅 [Information Schema](../reference/information_schema/loads.md)。

如果您提交了多个加载作业，可以根据与作业关联的 `LABEL` 进行筛选。示例：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'user_behavior';
```

在下面的输出中，加载作业 `user_behavior` 有两个条目：

- 第一条记录显示状态为 `CANCELLED`。滚动到 `ERROR_MSG`，您可以看到作业由于 `listPath failed` 而失败。
- 第二条记录显示状态为 `FINISHED`，表示作业已成功。

```Plaintext
JOB_ID|LABEL                                      |DATABASE_NAME|STATE    |PROGRESS           |TYPE  |PRIORITY|SCAN_ROWS|FILTERED_ROWS|UNSELECTED_ROWS|SINK_ROWS|ETL_INFO|TASK_INFO                                           |CREATE_TIME        |ETL_START_TIME     |ETL_FINISH_TIME    |LOAD_START_TIME    |LOAD_FINISH_TIME   |JOB_DETAILS                                                                                                                                                                                                                                                    |ERROR_MSG                             |TRACKING_URL|TRACKING_SQL|REJECTED_RECORD_PATH|
------+-------------------------------------------+-------------+---------+-------------------+------+--------+---------+-------------+---------------+---------+--------+----------------------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+------------+------------+--------------------+
 10121|user_behavior                              |mydatabase   |CANCELLED|ETL:N/A; LOAD:N/A  |BROKER|NORMAL  |        0|            0|              0|        0|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:59:30|                   |                   |                   |2023-08-10 14:59:34|{"All backends":{},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":0,"InternalTableLoadRows":0,"ScanBytes":0,"ScanRows":0,"TaskNumber":0,"Unfinished backends":{}}                                                                                        |type:ETL_RUN_FAIL; msg:listPath failed|            |            |                    |
 10106|user_behavior                              |mydatabase   |FINISHED |ETL:100%; LOAD:100%|BROKER|NORMAL  | 86953525|            0|              0| 86953525|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:50:15|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:50:19|2023-08-10 14:55:10|{"All backends":{"a5fe5e1d-d7d0-4826-ba99-c7348f9a5f2f":[10004]},"FileNumber":1,"FileSize":1225637388,"InternalTableLoadBytes":2710603082,"InternalTableLoadRows":86953525,"ScanBytes":1225637388,"ScanRows":86953525,"TaskNumber":1,"Unfinished backends":{"a5|                                      |            |            |                    |
```

确认加载作业已完成后，您可以检查目标表的子集以查看数据是否已成功加载。例子：

```SQL
SELECT * from user_behavior LIMIT 3;
```

返回如下查询结果，说明数据已成功加载：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|    142 | 2869980 |    2939262 | pv           | 2017-11-25 03:43:22 |
|    142 | 2522236 |    1669167 | pv           | 2017-11-25 15:14:12 |
|    142 | 3031639 |    3607361 | pv           | 2017-11-25 15:19:25 |
+--------+---------+------------+--------------+---------------------+
```

## 使用 Pipe

从 v3.2 开始，StarRocks 提供了 Pipe 加载方法，目前仅支持 Parquet 和 ORC 文件格式。

### Pipe 的优点

Pipe 适用于连续数据加载和大规模数据加载：

- **以微批次方式进行大规模数据加载有助于减少由数据错误引起的重试成本。**

  在 Pipe 的帮助下，StarRocks 可以高效地加载大量数据文件，这些文件的总数据量很大。Pipe 根据文件的数量或大小自动将文件拆分为较小的顺序任务，将加载作业分解为较小的连续任务。这种方法确保一个文件中的错误不会影响整个加载作业。Pipe 记录每个文件的加载状态，使您可以轻松识别和修复包含错误的文件。通过减少由于数据错误而需要重试的次数，这种方法有助于降低成本。

- **连续数据加载有助于减少人力成本。**

  Pipe 可以帮助您将新的或更新的数据文件写入特定位置，并持续将新数据从这些文件加载到 StarRocks 中。在创建具有指定 `"AUTO_INGEST" = "TRUE"` 的 Pipe 作业后，它将不断监视存储在指定路径中的数据文件的更改，并自动将数据文件中的新数据或更新的数据加载到目标 StarRocks 表中。

此外，Pipe 还执行文件唯一性检查，以防止重复数据加载。在加载过程中，Pipe 根据文件名和摘要检查每个数据文件的唯一性。如果具有特定文件名和摘要的文件已由 Pipe 作业处理过，则 Pipe 作业将跳过所有后续具有相同文件名和摘要的文件。请注意，像 AWS S3 这样的对象存储使用 `ETag` 作为文件摘要。

每个数据文件的加载状态记录并保存在 `information_schema.pipe_files` 视图中。在与视图关联的 Pipe 作业被删除后，有关在该作业中加载的文件的记录也将被删除。

### 数据流

![Pipe 数据流](../assets/pipe_data_flow.png)

### Pipe 和 INSERT+FILES() 之间的区别

Pipe 作业根据每个数据文件的大小和行数将其拆分为一个或多个事务。用户可以在加载过程中查询中间结果。相反，INSERT+`FILES()` 作业作为单个事务处理，并且用户无法在加载过程中查看数据。

### 文件加载顺序

对于每个 Pipe 作业，StarRocks 维护一个文件队列，从中获取并加载数据文件作为微批次。Pipe 不确保数据文件按照上传顺序加载。因此，新数据可能在旧数据之前加载。

### 典型例子

#### 创建数据库和表

创建数据库并切换到它：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手动创建一个表（我们建议表的架构与要从 AWS S3 加载的 Parquet 文件相同）：

```SQL
CREATE TABLE user_behavior_replica
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### 启动 Pipe 作业

运行以下命令以启动一个 Pipe 作业，该作业从示例数据集 `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` 中拉取数据并将其加载到 `user_behavior_replica` 表中：

```SQL
CREATE PIPE user_behavior_replica
PROPERTIES
(
    "AUTO_INGEST" = "TRUE"
)
AS
INSERT INTO user_behavior_replica
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
); 
```

> **笔记**
> 在上述命令中将您的凭据替换为 `AAA` 和 `BBB`。可以使用任何有效的 `aws.s3.access_key` 和 `aws.s3.secret_key`，因为任何经过 AWS 身份验证的用户都可以读取该对象。

此作业有三个主要部分：

- `pipe_name`：管道的名称。管道名称必须在管道所属的数据库中是唯一的。
- `INSERT_SQL`：用于从指定的源数据文件加载数据到目标表的 INSERT INTO SELECT FROM FILES 语句。
- `PROPERTIES`：一组可选参数，用于指定如何执行管道。这些参数包括 `AUTO_INGEST`、`POLL_INTERVAL`、`BATCH_SIZE` 和 `BATCH_FILES`。以 `"key" = "value"` 的格式指定这些属性。

有关详细的语法和参数描述，请参阅 [CREATE PIPE](../sql-reference/sql-statements/data-manipulation/CREATE_PIPE.md)。

#### 检查加载进度

- 使用 [SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md) 查询 Pipe 作业的进度。

  ```SQL
  SHOW PIPES;
  ```

  如果您提交了多个加载作业，可以根据与作业关联的 `NAME` 进行筛选。示例：

  ```SQL
  SHOW PIPES WHERE NAME = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR: NULL
   CREATED_TIME: 2023-11-09 15:35:01
  1 row in set (0.01 sec)
  ```

- 从 [`information_schema.pipes`](../reference/information_schema/pipes.md) 视图查询 Pipe 作业的进度。

  ```SQL
  SELECT * FROM information_schema.pipes;
  ```

  如果您提交了多个加载作业，可以根据与作业关联的 `PIPE_NAME` 进行筛选。示例：

  ```SQL
  SELECT * FROM information_schema.pipes WHERE pipe_name = 'user_behavior_replica' \G
  *************************** 1. row ***************************
  DATABASE_NAME: mydatabase
        PIPE_ID: 10217
      PIPE_NAME: user_behavior_replica
          STATE: RUNNING
     TABLE_NAME: mydatabase.user_behavior_replica
    LOAD_STATUS: {"loadedFiles":1,"loadedBytes":132251298,"loadingFiles":0,"lastLoadedTime":"2023-11-09 15:35:42"}
     LAST_ERROR:
   CREATED_TIME: 9891-01-15 07:51:45
  1 row in set (0.01 sec)
  ```

#### 检查文件状态

您可以从 [`information_schema.pipe_files`](../reference/information_schema/pipe_files.md) 视图查询从中加载的文件的加载状态。

```SQL
SELECT * FROM information_schema.pipe_files;
```

如果您提交了多个加载作业，可以根据与作业关联的 `PIPE_NAME` 进行筛选。示例：

```SQL
SELECT * FROM information_schema.pipe_files WHERE pipe_name = 'user_behavior_replica' \G
*************************** 1. row ***************************
   DATABASE_NAME: mydatabase
         PIPE_ID: 10217
       PIPE_NAME: user_behavior_replica
       FILE_NAME: s3://starrocks-datasets/user_behavior_ten_million_rows.parquet
    FILE_VERSION: e29daa86b1120fea58ad0d047e671787-8
       FILE_SIZE: 132251298
   LAST_MODIFIED: 2023-11-06 13:25:17
      LOAD_STATE: FINISHED
     STAGED_TIME: 2023-11-09 15:35:02
 START_LOAD_TIME: 2023-11-09 15:35:03
FINISH_LOAD_TIME: 2023-11-09 15:35:42
       ERROR_MSG:
1 row in set (0.03 sec)
```

#### 管理 Pipe 作业

您可以修改、暂停或恢复、删除或查询您创建的管道，并重试加载特定的数据文件。有关更多信息，请参阅 [ALTER PIPE](../sql-reference/sql-statements/data-manipulation/ALTER_PIPE.md)、[SUSPEND 或 RESUME PIPE](../sql-reference/sql-statements/data-manipulation/SUSPEND_or_RESUME_PIPE.md)、[DROP PIPE](../sql-reference/sql-statements/data-manipulation/DROP_PIPE.md)、[SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md) 和 [RETRY FILE](../sql-reference/sql-statements/data-manipulation/RETRY_FILE.md)。
```
手动创建一个表（我们建议该表与您要从 AWS S3 加载的 Parquet 文件具有相同的架构）：

```SQL
CREATE TABLE user_behavior_declared
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

创建表后，您可以使用 `INSERT INTO SELECT FROM FILES()` 加载它：

```SQL
INSERT INTO user_behavior_declared
SELECT * FROM FILES
(
    "path" = "s3://starrocks-datasets/user_behavior_ten_million_rows.parquet",
    "format" = "parquet",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
);
```

> **注意**
> 在上述命令中将您的凭据替换为 `AAA` 和 `BBB`。可以使用任何有效的 `aws.s3.access_key` 和 `aws.s3.secret_key`，因为该对象对任何经过 AWS 身份验证的用户都是可读的。

加载完成后，您可以查询表以验证数据是否已加载到其中。例如：

```SQL
SELECT * from user_behavior_declared LIMIT 3;
```

返回如下查询结果，说明数据加载成功：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|     58 | 4309692 |    1165503 | pv           | 2017-11-25 14:06:52 |
|     58 |  181489 |    1165503 | pv           | 2017-11-25 14:07:22 |
|     58 | 3722956 |    1165503 | pv           | 2017-11-25 14:09:28 |
+--------+---------+------------+--------------+---------------------+
```

#### 检查加载进度

您可以从 [`information_schema.loads`](../reference/information_schema/loads.md) 视图查询 INSERT 作业的进度。从 v3.1 开始支持此功能。例如：

```SQL
SELECT * FROM information_schema.loads ORDER BY JOB_ID DESC;
```

如果您提交了多个加载作业，您可以筛选与作业关联的 `LABEL`。例如：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'insert_e3b882f5-7eb3-11ee-ae77-00163e267b60' \G
*************************** 1. row ***************************
              JOB_ID: 10243
               LABEL: insert_e3b882f5-7eb3-11ee-ae77-00163e267b60
       DATABASE_NAME: mydatabase
               STATE: FINISHED
            PROGRESS: ETL:100%; LOAD:100%
                TYPE: INSERT
            PRIORITY: NORMAL
           SCAN_ROWS: 10000000
       FILTERED_ROWS: 0
     UNSELECTED_ROWS: 0
           SINK_ROWS: 10000000
            ETL_INFO:
           TASK_INFO: resource:N/A; timeout(s):300; max_filter_ratio:0.0
         CREATE_TIME: 2023-11-09 11:56:01
      ETL_START_TIME: 2023-11-09 11:56:01
     ETL_FINISH_TIME: 2023-11-09 11:56:01
     LOAD_START_TIME: 2023-11-09 11:56:01
    LOAD_FINISH_TIME: 2023-11-09 11:56:44
         JOB_DETAILS: {"All backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[10142]},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":311710786,"InternalTableLoadRows":10000000,"ScanBytes":581574034,"ScanRows":10000000,"TaskNumber":1,"Unfinished backends":{"e3b882f5-7eb3-11ee-ae77-00163e267b60":[]}}
           ERROR_MSG: NULL
        TRACKING_URL: NULL
        TRACKING_SQL: NULL
REJECTED_RECORD_PATH: NULL
```

有关 `loads` 视图中提供的字段的信息，请参阅[信息架构](../reference/information_schema/loads.md)。

> **注意**
> `INSERT` 是同步命令。如果 `INSERT` 作业仍在运行，则需要打开另一个会话来检查其执行状态。

## 使用 Broker Load

异步 Broker Load 过程负责与 S3 建立连接、提取数据并将数据存储在 StarRocks 中。

此方法支持 Parquet、ORC 和 CSV 文件格式。

### Broker Load 的优势

- Broker Load 支持在加载期间进行[数据转换](../loading/Etl_in_loading.md)和[数据变更，如 UPSERT 和 DELETE 操作](../loading/Load_to_Primary_Key_tables.md)。
- Broker Load 在后台运行，客户端无需保持连接即可继续作业。
- Broker Load 是长时间运行作业的首选，其默认超时时间为 4 小时。
- 除了 Parquet 和 ORC 文件格式外，Broker Load 还支持 CSV 文件。

### 数据流

![Broker Load 的工作流程](../assets/broker_load_how-to-work_en.png)

1. 用户创建加载作业。
2. 前端（FE）创建查询计划并将该计划分发到后端节点（BE）。
3. BE 从源中提取数据并将数据加载到 StarRocks 中。

### 典型示例

创建一个表，启动一个加载过程，从 S3 提取示例数据集 `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet`，并验证数据加载的进度和成功。

#### 创建数据库和表

创建数据库并切换到它：

```SQL
CREATE DATABASE IF NOT EXISTS mydatabase;
USE mydatabase;
```

手动创建一个表（我们建议该表与您要从 AWS S3 加载的 Parquet 文件具有相同的架构）：

```SQL
CREATE TABLE user_behavior
(
    UserID int(11),
    ItemID int(11),
    CategoryID int(11),
    BehaviorType varchar(65533),
    Timestamp datetime
)
ENGINE = OLAP 
DUPLICATE KEY(UserID)
DISTRIBUTED BY HASH(UserID)
PROPERTIES
(
    "replication_num" = "1"
);
```

#### 启动 Broker Load

运行以下命令启动 Broker Load 作业，将数据从示例数据集 `s3://starrocks-datasets/user_behavior_ten_million_rows.parquet` 加载到 `user_behavior` 表：

```SQL
LOAD LABEL user_behavior
(
    DATA INFILE("s3://starrocks-datasets/user_behavior_ten_million_rows.parquet")
    INTO TABLE user_behavior
    FORMAT AS "parquet"
 )
 WITH BROKER
 (
    "aws.s3.enable_ssl" = "true",
    "aws.s3.use_instance_profile" = "false",
    "aws.s3.region" = "us-east-1",
    "aws.s3.access_key" = "AAAAAAAAAAAAAAAAAAAA",
    "aws.s3.secret_key" = "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB"
 )
PROPERTIES
(
    "timeout" = "72000"
);
```

> **注意**
> 在上述命令中将您的凭据替换为 `AAA` 和 `BBB`。可以使用任何有效的 `aws.s3.access_key` 和 `aws.s3.secret_key`，因为该对象对任何经过 AWS 身份验证的用户都是可读的。

这项工作有四个主要部分：

- `LABEL`：查询加载作业状态时使用的字符串。
- `LOAD` 声明：源 URI、源数据格式和目标表名称。
- `BROKER`：源的连接详细信息。
- `PROPERTIES`：超时值和适用于加载作业的任何其他属性。

详细语法和参数说明请参见 [BROKER LOAD](../sql-reference/sql-statements/data-manipulation/BROKER_LOAD.md)。

#### 检查加载进度

您可以从 [`information_schema.loads`](../reference/information_schema/loads.md) 视图查询 Broker Load 作业的进度。从 v3.1 开始支持此功能。

```SQL
SELECT * FROM information_schema.loads;
```

有关 `loads` 视图中提供的字段的信息，请参阅 [信息架构](../reference/information_schema/loads.md)。

如果您提交了多个加载作业，您可以筛选与作业关联的 `LABEL`。例如：

```SQL
SELECT * FROM information_schema.loads WHERE LABEL = 'user_behavior';
```

在下面的输出中，有两个加载作业 `user_behavior` 条目：

- 第一条记录显示状态为 `CANCELLED`。滚动到 `ERROR_MSG`，您可以看到作业因 `listPath failed` 而失败。
- 第二条记录显示状态为 `FINISHED`，这意味着作业已成功。

```Plaintext
JOB_ID|LABEL                                      |DATABASE_NAME|STATE    |PROGRESS           |TYPE  |PRIORITY|SCAN_ROWS|FILTERED_ROWS|UNSELECTED_ROWS|SINK_ROWS|ETL_INFO|TASK_INFO                                           |CREATE_TIME        |ETL_START_TIME     |ETL_FINISH_TIME    |LOAD_START_TIME    |LOAD_FINISH_TIME   |JOB_DETAILS                                                                                                                                                                                                                                                    |ERROR_MSG                             |TRACKING_URL|TRACKING_SQL|REJECTED_RECORD_PATH|
------+-------------------------------------------+-------------+---------+-------------------+------+--------+---------+-------------+---------------+---------+--------+----------------------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+------------+------------+--------------------+
 10121|user_behavior                              |mydatabase   |CANCELLED|ETL:N/A; LOAD:N/A  |BROKER|NORMAL  |        0|            0|              0|        0|        |resource:N/A; timeout(s):72000; max_filter_ratio:0.0|2023-08-10 14:59:30|                   |                   |                   |2023-08-10 14:59:34|{"All backends":{},"FileNumber":0,"FileSize":0,"InternalTableLoadBytes":0,"InternalTableLoadRows":0,"ScanBytes":0,"ScanRows":0,"TaskNumber":0,"Unfinished backends":{}}                                                                                        |type:ETL_RUN_FAIL; msg:listPath failed|            |            |                    |
确认加载作业完成后，您可以检查目标表的子集以查看数据是否已成功加载。例子：

```SQL
SELECT * from user_behavior LIMIT 3;
```

返回如下查询结果，说明数据已成功加载：

```Plaintext
+--------+---------+------------+--------------+---------------------+
| UserID | ItemID  | CategoryID | BehaviorType | Timestamp           |
+--------+---------+------------+--------------+---------------------+
|    142 | 2869980 |    2939262 | pv           | 2017-11-25 03:43:22 |
|    142 | 2522236 |    1669167 | pv           | 2017-11-25 15:14:12 |
|    142 | 3031639 |    3607361 | pv           | 2017-11-25 15:19:25 |
+--------+---------+------------+--------------+---------------------+
```
#### 管理Pipe作业

您可以更改、暂停或恢复、删除或查询您已创建的Pipe，并重试加载特定的数据文件。有关更多信息，请参阅[ALTER PIPE](../sql-reference/sql-statements/data-manipulation/ALTER_PIPE.md)、[SUSPEND 或 RESUME PIPE](../sql-reference/sql-statements/data-manipulation/SUSPEND_or_RESUME_PIPE.md)、[DROP PIPE](../sql-reference/sql-statements/data-manipulation/DROP_PIPE.md)、[SHOW PIPES](../sql-reference/sql-statements/data-manipulation/SHOW_PIPES.md)和[RETRY FILE](../sql-reference/sql-statements/data-manipulation/RETRY_FILE.md)。